{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32cf57ae",
   "metadata": {},
   "source": [
    "# Generate a Quickstart Tutorial from a Documentation Site\n",
    "\n",
    "This script is and exrasise from \"Become an LLM Engineer in 8 weeks: Build and deploy 8 LLM apps, mastering Generative AI, RAG, LoRA and AI Agents.\" course.\n",
    "\n",
    "This solution is built to read a westite documentation and outputs a quick start, Roadmap to learn the tool, and a table of all neccary scripts and comands.\n",
    "\n",
    "Inputs: Documentation URL Link\n",
    "\n",
    "Output: \n",
    "1. Quick start\n",
    "2. Roadmap to learn the tool\n",
    "3. Table of all neccary scripts and comands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e6356e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# If these fail, please check you're running from an 'activated' environment with (llms) in the command prompt\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53506087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "# Initialize and constants\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key? Please visit the troubleshooting notebook!\")\n",
    "    \n",
    "MODEL = 'gpt-4o-mini'\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30972c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent a Webpage\n",
    "\n",
    "class Website:\n",
    "    \"\"\"\n",
    "    A utility class to represent a Website that we have scraped, now with links\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url)\n",
    "        self.body = response.content\n",
    "        soup = BeautifulSoup(self.body, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        if soup.body:\n",
    "            for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                irrelevant.decompose()\n",
    "            self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            self.text = \"\"\n",
    "        links = [link.get('href') for link in soup.find_all('a')]\n",
    "        self.links = [link for link in links if link]\n",
    "\n",
    "    def get_contents(self):\n",
    "        return f\"Webpage Title:\\n{self.title}\\nWebpage Contents:\\n{self.text}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af853742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/',\n",
       " '/models',\n",
       " '/datasets',\n",
       " '/spaces',\n",
       " '/docs',\n",
       " '/enterprise',\n",
       " '/pricing',\n",
       " '/login',\n",
       " '/join',\n",
       " '/docs/hub',\n",
       " '/docs/huggingface_hub',\n",
       " '/docs/huggingface.js',\n",
       " '/tasks',\n",
       " '/docs/dataset-viewer',\n",
       " '/docs/inference-providers',\n",
       " '/docs/inference-endpoints',\n",
       " '/docs/sagemaker',\n",
       " '/docs/text-generation-inference',\n",
       " '/docs/text-embeddings-inference',\n",
       " '/docs/transformers',\n",
       " '/docs/diffusers',\n",
       " '/docs/datasets',\n",
       " '/docs/transformers.js',\n",
       " '/docs/tokenizers',\n",
       " '/docs/evaluate',\n",
       " '/docs/timm',\n",
       " 'https://sbert.net/',\n",
       " '/docs/peft',\n",
       " '/docs/accelerate',\n",
       " '/docs/optimum',\n",
       " '/docs/optimum-neuron',\n",
       " '/docs/trl',\n",
       " '/docs/safetensors',\n",
       " '/docs/bitsandbytes',\n",
       " '/docs/lighteval',\n",
       " 'https://www.gradio.app/docs/',\n",
       " '/docs/smolagents',\n",
       " '/docs/autotrain',\n",
       " '/docs/chat-ui',\n",
       " '/docs/leaderboards',\n",
       " 'https://argilla-io.github.io/argilla/',\n",
       " 'https://distilabel.argilla.io/',\n",
       " '/blog',\n",
       " '/learn',\n",
       " '/join/discord',\n",
       " 'https://discuss.huggingface.co/',\n",
       " 'https://github.com/huggingface',\n",
       " '/terms-of-service',\n",
       " '/privacy',\n",
       " '/huggingface',\n",
       " 'https://apply.workable.com/huggingface/',\n",
       " '/',\n",
       " '/models',\n",
       " '/datasets',\n",
       " '/spaces',\n",
       " '/pricing',\n",
       " '/docs']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "website = Website(\"https://huggingface.co/docs\")\n",
    "website.links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8d5fa1",
   "metadata": {},
   "source": [
    "## First step: Have GPT-4o-mini figure out which links are relevant\n",
    "\n",
    "### Use a call to gpt-4o-mini to read the links on a webpage, and respond in structured JSON.  \n",
    "It should decide which links are relevant, and replace relative links such as \"/about\" with \"https://company.com/about\".  \n",
    "We will use \"one shot prompting\" in which we provide an example of how it should respond in the prompt.\n",
    "\n",
    "This is an excellent use case for an LLM, because it requires nuanced understanding. Imagine trying to code this without LLMs by parsing and analyzing the webpage - it would be very hard!\n",
    "\n",
    "Sidenote: there is a more advanced technique called \"Structured Outputs\" in which we require the model to respond according to a spec. We cover this technique in Week 8 during our autonomous Agentic AI project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb495bc",
   "metadata": {},
   "source": [
    "### System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1700314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_system_prompt = \"You are provided with a list of links found on a documentation webpage. \\\n",
    "You are able to decide which of the links would be most relevant to include in a beginners guide to a user.\\n\"\n",
    "link_system_prompt += \"You should respond in JSON as in this example:\"\n",
    "link_system_prompt += \"\"\"\n",
    "{\n",
    "    \"links\": [\n",
    "        {\"type\": \"Datasets\", \"url\": \"https://huggingface.co/docs/datasets/index\"},\n",
    "        {\"type\": \"Installation\": \"url\": \"https://flask.palletsprojects.com/en/stable/installation/\"}\n",
    "        {\"type\": \"introduction\": \"url\": \"https://www.tensorflow.org/learn\"}\n",
    "    ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b30f4434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are provided with a list of links found on a documentation webpage. You are able to decide which of the links would be most relevant to include in a beginners guide to a user.\n",
      "You should respond in JSON as in this example:\n",
      "{\n",
      "    \"links\": [\n",
      "        {\"type\": \"Datasets\", \"url\": \"https://huggingface.co/docs/datasets/index\"},\n",
      "        {\"type\": \"Installation\": \"url\": \"https://flask.palletsprojects.com/en/stable/installation/\"}\n",
      "        {\"type\": \"introduction\": \"url\": \"https://www.tensorflow.org/learn\"}\n",
      "    ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(link_system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde9cc58",
   "metadata": {},
   "source": [
    "### User Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f3b2bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_user_prompt(website):\n",
    "    user_prompt = f\"Here is the list of links on the website of {website.url} - \"\n",
    "    user_prompt += \"please decide which of these are relevant web links for a beginners guide tutorial, respond with the full https URL in JSON format. \\\n",
    "Do not include Terms of Service, Privacy, email links.\\n\"\n",
    "    user_prompt += \"Links (some might be relative links):\\n\"\n",
    "    user_prompt += \"\\n\".join(website.links)\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "de2f50cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the list of links on the website of https://huggingface.co/docs - please decide which of these are relevant web links for a beginners guide tutorial, respond with the full https URL in JSON format. Do not include Terms of Service, Privacy, email links.\n",
      "Links (some might be relative links):\n",
      "/\n",
      "/models\n",
      "/datasets\n",
      "/spaces\n",
      "/docs\n",
      "/enterprise\n",
      "/pricing\n",
      "/login\n",
      "/join\n",
      "/docs/hub\n",
      "/docs/huggingface_hub\n",
      "/docs/huggingface.js\n",
      "/tasks\n",
      "/docs/dataset-viewer\n",
      "/docs/inference-providers\n",
      "/docs/inference-endpoints\n",
      "/docs/sagemaker\n",
      "/docs/text-generation-inference\n",
      "/docs/text-embeddings-inference\n",
      "/docs/transformers\n",
      "/docs/diffusers\n",
      "/docs/datasets\n",
      "/docs/transformers.js\n",
      "/docs/tokenizers\n",
      "/docs/evaluate\n",
      "/docs/timm\n",
      "https://sbert.net/\n",
      "/docs/peft\n",
      "/docs/accelerate\n",
      "/docs/optimum\n",
      "/docs/optimum-neuron\n",
      "/docs/trl\n",
      "/docs/safetensors\n",
      "/docs/bitsandbytes\n",
      "/docs/lighteval\n",
      "https://www.gradio.app/docs/\n",
      "/docs/smolagents\n",
      "/docs/autotrain\n",
      "/docs/chat-ui\n",
      "/docs/leaderboards\n",
      "https://argilla-io.github.io/argilla/\n",
      "https://distilabel.argilla.io/\n",
      "/blog\n",
      "/learn\n",
      "/join/discord\n",
      "https://discuss.huggingface.co/\n",
      "https://github.com/huggingface\n",
      "/terms-of-service\n",
      "/privacy\n",
      "/huggingface\n",
      "https://apply.workable.com/huggingface/\n",
      "/\n",
      "/models\n",
      "/datasets\n",
      "/spaces\n",
      "/pricing\n",
      "/docs\n"
     ]
    }
   ],
   "source": [
    "print(get_links_user_prompt(website))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8033e3ec",
   "metadata": {},
   "source": [
    "### Get the Relevant Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "67b3fe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(url):\n",
    "    website = Website(url)\n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": link_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_links_user_prompt(website)}\n",
    "      ],\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    return json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c0c54e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'links': [{'type': 'Models', 'url': 'https://huggingface.co/models'},\n",
       "  {'type': 'Datasets', 'url': 'https://huggingface.co/docs/datasets'},\n",
       "  {'type': 'Spaces', 'url': 'https://huggingface.co/spaces'},\n",
       "  {'type': 'Documentation', 'url': 'https://huggingface.co/docs'},\n",
       "  {'type': 'Hugging Face Hub',\n",
       "   'url': 'https://huggingface.co/docs/huggingface_hub'},\n",
       "  {'type': 'Tasks', 'url': 'https://huggingface.co/tasks'},\n",
       "  {'type': 'Inference Providers',\n",
       "   'url': 'https://huggingface.co/docs/inference-providers'},\n",
       "  {'type': 'Text Generation Inference',\n",
       "   'url': 'https://huggingface.co/docs/text-generation-inference'},\n",
       "  {'type': 'Transformers Documentation',\n",
       "   'url': 'https://huggingface.co/docs/transformers'},\n",
       "  {'type': 'Datasets Viewer',\n",
       "   'url': 'https://huggingface.co/docs/dataset-viewer'},\n",
       "  {'type': 'Evaluation', 'url': 'https://huggingface.co/docs/evaluate'},\n",
       "  {'type': 'Learn', 'url': 'https://huggingface.co/learn'},\n",
       "  {'type': 'Chat UI', 'url': 'https://huggingface.co/docs/chat-ui'}]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_links(\"https://huggingface.co/docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91057efc",
   "metadata": {},
   "source": [
    "## Second step: make the Quickstart!\n",
    "\n",
    "Assemble all the details into another prompt to GPT4-o\n",
    "\n",
    "this will output:\n",
    "1. Quick start\n",
    "2. Roadmap to learn the tool\n",
    "3. Table of all neccary scripts and comands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "460a40f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_details(url):\n",
    "    result = \"Landing page:\\n\"\n",
    "    result += Website(url).get_contents()\n",
    "    links = get_links(url)\n",
    "    print(\"Found links:\", links)\n",
    "    for link in links[\"links\"]:\n",
    "        result += f\"\\n\\n{link['type']}\\n\"\n",
    "        result += Website(link[\"url\"]).get_contents()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "83c83d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found links: {'links': [{'type': 'Models', 'url': 'https://huggingface.co/models'}, {'type': 'Datasets', 'url': 'https://huggingface.co/docs/datasets'}, {'type': 'Spaces', 'url': 'https://huggingface.co/spaces'}, {'type': 'Documentation', 'url': 'https://huggingface.co/docs'}, {'type': 'Hub Documentation', 'url': 'https://huggingface.co/docs/huggingface_hub'}, {'type': 'Tasks', 'url': 'https://huggingface.co/docs/tasks'}, {'type': 'Dataset Viewer', 'url': 'https://huggingface.co/docs/dataset-viewer'}, {'type': 'Text Generation Inference', 'url': 'https://huggingface.co/docs/text-generation-inference'}, {'type': 'Transformers', 'url': 'https://huggingface.co/docs/transformers'}, {'type': 'Diffusers', 'url': 'https://huggingface.co/docs/diffusers'}, {'type': 'Tokenizers', 'url': 'https://huggingface.co/docs/tokenizers'}, {'type': 'Evaluate', 'url': 'https://huggingface.co/docs/evaluate'}, {'type': 'Accelerate', 'url': 'https://huggingface.co/docs/accelerate'}, {'type': 'Autotrain', 'url': 'https://huggingface.co/docs/autotrain'}, {'type': 'Chat UI', 'url': 'https://huggingface.co/docs/chat-ui'}, {'type': 'Learn', 'url': 'https://huggingface.co/learn'}, {'type': 'Blog', 'url': 'https://huggingface.co/blog'}]}\n",
      "Landing page:\n",
      "Webpage Title:\n",
      "Hugging Face - Documentation\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Documentations\n",
      "Hub & Client Libraries\n",
      "Hub\n",
      "Host Git-based models, datasets, and Spaces on the HF Hub\n",
      "Hub Python Library\n",
      "Python client to interact with the Hugging Face Hub\n",
      "Huggingface.js\n",
      "JavaScript libraries for Hugging Face with built-in TS types\n",
      "Tasks\n",
      "Explore demos, models, and datasets for any ML tasks\n",
      "Dataset viewer\n",
      "API for metadata, stats, and content of HF Hub datasets\n",
      "Deployment & Inference\n",
      "Inference Providers\n",
      "Call 200k+ models hosted by our 10+ Inference partners\n",
      "Inference Endpoints (dedicated)\n",
      "Deploy models on dedicated & fully managed infrastructure on HF\n",
      "Amazon SageMaker\n",
      "Train/deploy Transformers models with SageMaker/HF DLCs\n",
      "Text Generation Inference\n",
      "Serve language models with TGI optimized toolkit\n",
      "Text Embeddings Inference\n",
      "Serve embeddings models with TEI optimized toolkit\n",
      "Core ML Libraries\n",
      "Transformers\n",
      "State-of-the-art ML for PyTorch, TensorFlow, JAX\n",
      "Diffusers\n",
      "State-of-the-art Diffusion models in PyTorch\n",
      "Datasets\n",
      "Access & share datasets for any ML tasks\n",
      "Transformers.js\n",
      "State-of-the-art ML running directly in your browser\n",
      "Tokenizers\n",
      "Fast tokenizers optimized for research & production\n",
      "Evaluate\n",
      "Evaluate and compare models performance\n",
      "timm\n",
      "State-of-the-art vision models: layers, optimizers, and utilities\n",
      "Sentence Transformers\n",
      "Embeddings, Retrieval, and Reranking\n",
      "Training & Optimization\n",
      "PEFT\n",
      "Parameter-efficient finetuning for large language models\n",
      "Accelerate\n",
      "Train PyTorch models with multi-GPU, TPU, mixed precision\n",
      "Optimum\n",
      "Optimize HF Transformers for faster training/inference\n",
      "AWS Trainium & Inferentia\n",
      "Train/deploy Transformers/Diffusers on AWS\n",
      "TRL\n",
      "Train transformers LMs with reinforcement learning\n",
      "Safetensors\n",
      "Safe way to store/distribute neural network weights\n",
      "Bitsandbytes\n",
      "Optimize and quantize models with bitsandbytes\n",
      "Lighteval\n",
      "All-in-one toolkit to evaluate LLMs across multiple backends\n",
      "Collaboration & Extras\n",
      "Gradio\n",
      "Build ML demos and web apps with a few lines of Python\n",
      "smolagents\n",
      "Smol library to build great agents in Python\n",
      "AutoTrain\n",
      "AutoTrain API and UI for seamless model training\n",
      "Chat UI\n",
      "Open source chat frontend powering HuggingChat\n",
      "Leaderboards\n",
      "Create custom Leaderboards on Hugging Face\n",
      "Argilla\n",
      "Collaboration tool for building high-quality datasets\n",
      "Distilabel\n",
      "Framework for synthetic data generation and AI feedback\n",
      "Community\n",
      "Blog\n",
      "Learn\n",
      "Discord\n",
      "Forum\n",
      "Github\n",
      "System theme\n",
      "Company\n",
      "TOS\n",
      "Privacy\n",
      "About\n",
      "Jobs\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Pricing\n",
      "Docs\n",
      "\n",
      "\n",
      "\n",
      "Models\n",
      "Webpage Title:\n",
      "Models - Hugging Face\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Edit Models filters\n",
      "Tasks\n",
      "Libraries\n",
      "Datasets\n",
      "Languages\n",
      "Licenses\n",
      "Other\n",
      "Multimodal\n",
      "Audio-Text-to-Text\n",
      "Image-Text-to-Text\n",
      "Visual Question Answering\n",
      "Document Question Answering\n",
      "Video-Text-to-Text\n",
      "Visual Document Retrieval\n",
      "Any-to-Any\n",
      "Computer Vision\n",
      "Depth Estimation\n",
      "Image Classification\n",
      "Object Detection\n",
      "Image Segmentation\n",
      "Text-to-Image\n",
      "Image-to-Text\n",
      "Image-to-Image\n",
      "Image-to-Video\n",
      "Unconditional Image Generation\n",
      "Video Classification\n",
      "Text-to-Video\n",
      "Zero-Shot Image Classification\n",
      "Mask Generation\n",
      "Zero-Shot Object Detection\n",
      "Text-to-3D\n",
      "Image-to-3D\n",
      "Image Feature Extraction\n",
      "Keypoint Detection\n",
      "Natural Language Processing\n",
      "Text Classification\n",
      "Token Classification\n",
      "Table Question Answering\n",
      "Question Answering\n",
      "Zero-Shot Classification\n",
      "Translation\n",
      "Summarization\n",
      "Feature Extraction\n",
      "Text Generation\n",
      "Text2Text Generation\n",
      "Fill-Mask\n",
      "Sentence Similarity\n",
      "Text Ranking\n",
      "Audio\n",
      "Text-to-Speech\n",
      "Text-to-Audio\n",
      "Automatic Speech Recognition\n",
      "Audio-to-Audio\n",
      "Audio Classification\n",
      "Voice Activity Detection\n",
      "Tabular\n",
      "Tabular Classification\n",
      "Tabular Regression\n",
      "Time Series Forecasting\n",
      "Reinforcement Learning\n",
      "Reinforcement Learning\n",
      "Robotics\n",
      "Other\n",
      "Graph Machine Learning\n",
      "Apply filters\n",
      "Models\n",
      "Full-text search\n",
      "Add filters\n",
      "Sort: \n",
      "\t\tTrending\n",
      "Wan-AI/Wan2.1-VACE-14B\n",
      "Image-to-Video\n",
      "•\n",
      "Updated\n",
      "2 days ago\n",
      "•\n",
      "4.02k\n",
      "•\n",
      "240\n",
      "multimodalart/isometric-skeumorphic-3d-bnb\n",
      "Text-to-Image\n",
      "•\n",
      "Updated\n",
      "6 days ago\n",
      "•\n",
      "849\n",
      "•\n",
      "•\n",
      "227\n",
      "nvidia/parakeet-tdt-0.6b-v2\n",
      "Automatic Speech Recognition\n",
      "•\n",
      "Updated\n",
      "5 days ago\n",
      "•\n",
      "243k\n",
      "•\n",
      "1.01k\n",
      "nari-labs/Dia-1.6B\n",
      "Text-to-Speech\n",
      "•\n",
      "Updated\n",
      "7 days ago\n",
      "•\n",
      "173k\n",
      "•\n",
      "•\n",
      "2.3k\n",
      "google/gemma-3n-E4B-it-litert-preview\n",
      "Image-Text-to-Text\n",
      "•\n",
      "Updated\n",
      "about 16 hours ago\n",
      "•\n",
      "171\n",
      "stabilityai/stable-audio-open-small\n",
      "Text-to-Audio\n",
      "•\n",
      "Updated\n",
      "7 days ago\n",
      "•\n",
      "1.07k\n",
      "•\n",
      "164\n",
      "lodestones/Chroma\n",
      "Text-to-Image\n",
      "•\n",
      "Updated\n",
      "3 days ago\n",
      "•\n",
      "630\n",
      "a-m-team/AM-Thinking-v1\n",
      "Text Generation\n",
      "•\n",
      "Updated\n",
      "7 days ago\n",
      "•\n",
      "840\n",
      "•\n",
      "•\n",
      "170\n",
      "ByteDance-Seed/BAGEL-7B-MoT\n",
      "Updated\n",
      "about 8 hours ago\n",
      "•\n",
      "125\n",
      "IndexTeam/Index-anisora\n",
      "Updated\n",
      "2 days ago\n",
      "•\n",
      "33\n",
      "•\n",
      "124\n",
      "Lightricks/LTX-Video\n",
      "Text-to-Video\n",
      "•\n",
      "Updated\n",
      "about 23 hours ago\n",
      "•\n",
      "294k\n",
      "•\n",
      "•\n",
      "1.56k\n",
      "Intelligent-Internet/II-Medical-8B\n",
      "Text Generation\n",
      "•\n",
      "Updated\n",
      "6 days ago\n",
      "•\n",
      "1.25k\n",
      "•\n",
      "•\n",
      "119\n",
      "ByteDance/Dolphin\n",
      "Image-Text-to-Text\n",
      "•\n",
      "Updated\n",
      "about 9 hours ago\n",
      "•\n",
      "15\n",
      "•\n",
      "91\n",
      "black-forest-labs/FLUX.1-dev\n",
      "Text-to-Image\n",
      "•\n",
      "Updated\n",
      "Aug 16, 2024\n",
      "•\n",
      "2.64M\n",
      "•\n",
      "•\n",
      "10.3k\n",
      "google/medgemma-27b-text-it\n",
      "Image-Text-to-Text\n",
      "•\n",
      "Updated\n",
      "about 18 hours ago\n",
      "•\n",
      "82\n",
      "facebook/OMol25\n",
      "Updated\n",
      "1 day ago\n",
      "•\n",
      "83\n",
      "facebook/KernelLLM\n",
      "Text Generation\n",
      "•\n",
      "Updated\n",
      "1 day ago\n",
      "•\n",
      "252\n",
      "•\n",
      "78\n",
      "Qwen/Qwen3-235B-A22B\n",
      "Text Generation\n",
      "•\n",
      "Updated\n",
      "about 5 hours ago\n",
      "•\n",
      "125k\n",
      "•\n",
      "•\n",
      "870\n",
      "google/medgemma-4b-it\n",
      "Image-Text-to-Text\n",
      "•\n",
      "Updated\n",
      "about 13 hours ago\n",
      "•\n",
      "75\n",
      "BLIP3o/BLIP3o-Model\n",
      "Updated\n",
      "about 20 hours ago\n",
      "•\n",
      "5.58k\n",
      "•\n",
      "72\n",
      "hexgrad/Kokoro-82M\n",
      "Text-to-Speech\n",
      "•\n",
      "Updated\n",
      "Apr 10\n",
      "•\n",
      "1.59M\n",
      "•\n",
      "•\n",
      "4.37k\n",
      "deepseek-ai/DeepSeek-R1\n",
      "Text Generation\n",
      "•\n",
      "Updated\n",
      "Mar 27\n",
      "•\n",
      "667k\n",
      "•\n",
      "•\n",
      "12.2k\n",
      "openbmb/AgentCPM-GUI\n",
      "Image-Text-to-Text\n",
      "•\n",
      "Updated\n",
      "6 days ago\n",
      "•\n",
      "344\n",
      "•\n",
      "106\n",
      "TEN-framework/ten-vad\n",
      "Voice Activity Detection\n",
      "•\n",
      "Updated\n",
      "about 4 hours ago\n",
      "•\n",
      "22\n",
      "•\n",
      "66\n",
      "OuteAI/OuteTTS-1.0-0.6B\n",
      "Text-to-Speech\n",
      "•\n",
      "Updated\n",
      "3 days ago\n",
      "•\n",
      "282\n",
      "•\n",
      "63\n",
      "ACE-Step/ACE-Step-v1-3.5B\n",
      "Text-to-Audio\n",
      "•\n",
      "Updated\n",
      "8 days ago\n",
      "•\n",
      "453\n",
      "google/gemma-3n-E2B-it-litert-preview\n",
      "Image-Text-to-Text\n",
      "•\n",
      "Updated\n",
      "about 15 hours ago\n",
      "•\n",
      "61\n",
      "ZuluVision/MoviiGen1.1\n",
      "Text-to-Video\n",
      "•\n",
      "Updated\n",
      "1 day ago\n",
      "•\n",
      "412\n",
      "•\n",
      "60\n",
      "Kijai/WanVideo_comfy\n",
      "Updated\n",
      "6 days ago\n",
      "•\n",
      "551\n",
      "stepfun-ai/Step1X-3D\n",
      "Updated\n",
      "8 days ago\n",
      "•\n",
      "73\n",
      "System theme\n",
      "Company\n",
      "TOS\n",
      "Privacy\n",
      "About\n",
      "Jobs\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Pricing\n",
      "Docs\n",
      "\n",
      "\n",
      "\n",
      "Datasets\n",
      "Webpage Title:\n",
      "Datasets\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Datasets documentation\n",
      "Datasets\n",
      "Datasets\n",
      "🏡 View all docs\n",
      "AWS Trainium & Inferentia\n",
      "Accelerate\n",
      "Amazon SageMaker\n",
      "Argilla\n",
      "AutoTrain\n",
      "Bitsandbytes\n",
      "Chat UI\n",
      "Dataset viewer\n",
      "Datasets\n",
      "Diffusers\n",
      "Distilabel\n",
      "Evaluate\n",
      "Gradio\n",
      "Hub\n",
      "Hub Python Library\n",
      "Huggingface.js\n",
      "Inference Endpoints (dedicated)\n",
      "Inference Providers\n",
      "Leaderboards\n",
      "Lighteval\n",
      "Optimum\n",
      "PEFT\n",
      "Safetensors\n",
      "Sentence Transformers\n",
      "TRL\n",
      "Tasks\n",
      "Text Embeddings Inference\n",
      "Text Generation Inference\n",
      "Tokenizers\n",
      "Transformers\n",
      "Transformers.js\n",
      "smolagents\n",
      "timm\n",
      "Search documentation\n",
      "main\n",
      "v3.6.0\n",
      "v3.5.1\n",
      "v3.4.1\n",
      "v3.3.2\n",
      "v3.2.0\n",
      "v3.1.0\n",
      "v3.0.2\n",
      "v2.21.0\n",
      "v2.20.0\n",
      "v2.19.0\n",
      "v2.18.0\n",
      "v2.17.1\n",
      "v2.16.1\n",
      "v2.15.0\n",
      "v2.14.7\n",
      "v2.13.2\n",
      "v2.12.0\n",
      "v2.11.0\n",
      "v2.10.0\n",
      "v2.9.0\n",
      "v2.8.0\n",
      "v2.7.1\n",
      "v2.6.2\n",
      "v2.5.2\n",
      "v2.4.0\n",
      "v2.3.2\n",
      "v2.2.1\n",
      "v2.1.0\n",
      "v2.0.0\n",
      "v1.18.3\n",
      "v1.17.0\n",
      "v1.16.1\n",
      "v1.15.1\n",
      "v1.14.0\n",
      "v1.13.3\n",
      "v1.12.1\n",
      "v1.11.0\n",
      "v1.10.2\n",
      "v1.9.0\n",
      "v1.8.0\n",
      "v1.7.0\n",
      "v1.6.2\n",
      "v1.5.0\n",
      "v1.4.1\n",
      "v1.3.0\n",
      "v1.2.1\n",
      "v1.1.3\n",
      "v1.0.2\n",
      "v0.4.0\n",
      "v0.3.0\n",
      "EN\n",
      "Get started\n",
      "🤗 Datasets\n",
      "Quickstart\n",
      "Installation\n",
      "Tutorials\n",
      "Overview\n",
      "Load a dataset from the Hub\n",
      "Know your dataset\n",
      "Preprocess\n",
      "Create a dataset\n",
      "Share a dataset to the Hub\n",
      "How-to guides\n",
      "Overview\n",
      "General usage\n",
      "Load\n",
      "Process\n",
      "Stream\n",
      "Use with PyTorch\n",
      "Use with TensorFlow\n",
      "Use with NumPy\n",
      "Use with JAX\n",
      "Use with Pandas\n",
      "Use with Polars\n",
      "Use with PyArrow\n",
      "Use with Spark\n",
      "Cache management\n",
      "Cloud storage\n",
      "Search index\n",
      "CLI\n",
      "Troubleshooting\n",
      "Audio\n",
      "Load audio data\n",
      "Process audio data\n",
      "Create an audio dataset\n",
      "Vision\n",
      "Load image data\n",
      "Process image data\n",
      "Create an image dataset\n",
      "Depth estimation\n",
      "Image classification\n",
      "Semantic segmentation\n",
      "Object detection\n",
      "Load video data\n",
      "Create a video dataset\n",
      "Load document data\n",
      "Create a document dataset\n",
      "Text\n",
      "Load text data\n",
      "Process text data\n",
      "Tabular\n",
      "Load tabular data\n",
      "Dataset repository\n",
      "Share\n",
      "Create a dataset card\n",
      "Structure your repository\n",
      "Create a dataset loading script\n",
      "Conceptual guides\n",
      "Datasets 🤝 Arrow\n",
      "The cache\n",
      "Dataset or IterableDataset\n",
      "Dataset features\n",
      "Build and load\n",
      "Batch mapping\n",
      "Reference\n",
      "Main classes\n",
      "Builder classes\n",
      "Loading methods\n",
      "Table Classes\n",
      "Utilities\n",
      "Join the Hugging Face community\n",
      "and get access to the augmented documentation experience\n",
      "Collaborate on models, datasets and Spaces\n",
      "Faster examples with accelerated inference\n",
      "Switch between documentation themes\n",
      "Sign Up\n",
      "to get started\n",
      "Datasets\n",
      "🤗 Datasets is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks.\n",
      "Load a dataset in a single line of code, and use our powerful data processing methods to quickly get your dataset ready for training in a deep learning model. Backed by the Apache Arrow format, process large datasets with zero-copy reads without any memory constraints for optimal speed and efficiency. We also feature a deep integration with the\n",
      "Hugging Face Hub\n",
      ", allowing you to easily load and share a dataset with the wider machine learning community.\n",
      "Find your dataset today on the\n",
      "Hugging Face Hub\n",
      ", and take an in-depth look inside of it with the live viewer.\n",
      "Tutorials\n",
      "Learn the basics and become familiar with loading, accessing, and processing a dataset. Start here if you are using 🤗 Datasets for the first time!\n",
      "How-to guides\n",
      "Practical guides to help you achieve a specific goal. Take a look at these guides to learn how to use 🤗 Datasets to solve real-world problems.\n",
      "Conceptual guides\n",
      "High-level explanations for building a better understanding about important topics such as the underlying data format, the cache, and how datasets are generated.\n",
      "Reference\n",
      "Technical descriptions of how 🤗 Datasets classes and methods work.\n",
      "<\n",
      ">\n",
      "Update\n",
      "on GitHub\n",
      "Quickstart\n",
      "→\n",
      "Datasets\n",
      "\n",
      "\n",
      "\n",
      "Spaces\n",
      "Webpage Title:\n",
      "Spaces - Hugging Face\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Spaces\n",
      "·\n",
      "The AI App Directory\n",
      "New Space\n",
      "What is Spaces?\n",
      "Image Generation\n",
      "Video Generation\n",
      "Text Generation\n",
      "Language Translation\n",
      "Speech Synthesis\n",
      "3D Modeling\n",
      "Object Detection\n",
      "Text Analysis\n",
      "Image Editing\n",
      "Code Generation\n",
      "Question Answering\n",
      "Data Visualization\n",
      "Voice Cloning\n",
      "Background Removal\n",
      "Image Upscaling\n",
      "OCR\n",
      "Document Analysis\n",
      "Visual QA\n",
      "Image Captioning\n",
      "Chatbots\n",
      "Sentiment Analysis\n",
      "Text Summarization\n",
      "Music Generation\n",
      "Medical Imaging\n",
      "Financial Analysis\n",
      "Game AI\n",
      "Model Benchmarking\n",
      "Fine Tuning Tools\n",
      "Dataset Creation\n",
      "Pose Estimation\n",
      "Face Recognition\n",
      "Anomaly Detection\n",
      "Recommendation Systems\n",
      "Character Animation\n",
      "Style Transfer\n",
      "Image\n",
      "Spaces of the week\n",
      "19 May 2025\n",
      "Sort: \n",
      "\t\tRelevance\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "310\n",
      "LTX Video Fast\n",
      "🎥\n",
      "ultra-fast video model, LTX 0.9.7 13B distilled\n",
      "Lightricks\n",
      "3 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "166\n",
      "Joy Caption Beta One\n",
      "🖼\n",
      "Generate descriptive captions for images based on various styles\n",
      "fancyfeast\n",
      "9 days ago\n",
      "Running\n",
      "103\n",
      "SmolVLM realtime WebGPU\n",
      "⚡\n",
      "Generate descriptions based on camera input\n",
      "webml-community\n",
      "7 days ago\n",
      "Running\n",
      "86\n",
      "Seed1.5 VL\n",
      "🚀\n",
      "Seed1.5-VL API Demo\n",
      "ByteDance-Seed\n",
      "7 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "153\n",
      "Step1X 3D\n",
      "🐨\n",
      "image2mesh\n",
      "stepfun-ai\n",
      "7 days ago\n",
      "Running\n",
      "24\n",
      "DiffVox\n",
      "🦀\n",
      "Enhance vocal audio with professional effects\n",
      "yoyolicoris\n",
      "about 19 hours ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "32\n",
      "Daily Paper Podcast\n",
      "🎙\n",
      "Generates a podcast about today's top trending paper.\n",
      "fdaudens\n",
      "about 15 hours ago\n",
      "Running\n",
      "20\n",
      "Surf Spot Finder\n",
      "🏄\n",
      "Find a surf spot near you\n",
      "mozilla-ai\n",
      "about 23 hours ago\n",
      "All running apps, trending first\n",
      "Running\n",
      "6.89k\n",
      "DeepSite\n",
      "🐳\n",
      "Generate any application with DeepSeek\n",
      "enzostvs\n",
      "11 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "310\n",
      "LTX Video Fast\n",
      "🎥\n",
      "ultra-fast video model, LTX 0.9.7 13B distilled\n",
      "Lightricks\n",
      "3 days ago\n",
      "Running\n",
      "508\n",
      "FLUX Pro Unlimited\n",
      "🔥\n",
      "Use the FLUX-Pro model as much as you want.\n",
      "NihalGazi\n",
      "5 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "443\n",
      "DreamO\n",
      "🐨\n",
      "A Unified Framework for Image Customization\n",
      "ByteDance\n",
      "10 days ago\n",
      "Running\n",
      "on\n",
      "CPU Upgrade\n",
      "716\n",
      "Computer Agent\n",
      "🖥\n",
      "Interact with an AI agent to perform web tasks\n",
      "smolagents\n",
      "7 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "153\n",
      "Step1X 3D\n",
      "🐨\n",
      "image2mesh\n",
      "stepfun-ai\n",
      "7 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "166\n",
      "Joy Caption Beta One\n",
      "🖼\n",
      "Generate descriptive captions for images based on various styles\n",
      "fancyfeast\n",
      "9 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "416\n",
      "ACE Step\n",
      "😻\n",
      "A Step Towards Music Generation Foundation Model\n",
      "ACE-Step\n",
      "5 days ago\n",
      "Running\n",
      "103\n",
      "SmolVLM realtime WebGPU\n",
      "⚡\n",
      "Generate descriptions based on camera input\n",
      "webml-community\n",
      "7 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "1.39k\n",
      "Dia 1.6B\n",
      "👯\n",
      "Generate realistic dialogue from a script, using Dia!\n",
      "nari-labs\n",
      "16 days ago\n",
      "Running\n",
      "on\n",
      "CPU Upgrade\n",
      "8.79k\n",
      "Kolors Virtual Try-On\n",
      "👕\n",
      "Try on clothes by uploading images\n",
      "Kwai-Kolors\n",
      "Sep 18, 2024\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "164\n",
      "FramePack F1\n",
      "📹\n",
      "fast video generation from images & text\n",
      "linoyts\n",
      "about 5 hours ago\n",
      "Running\n",
      "116\n",
      "Realistic Text To Speech Unlimited\n",
      "🔥\n",
      "Free Text-To-Speech generator with Emotion control (OpenAI)\n",
      "NihalGazi\n",
      "2 days ago\n",
      "Running\n",
      "82\n",
      "MiniMax Speech Tech Report\n",
      "🎙\n",
      "Generate high-quality speech from text with voice cloning\n",
      "MiniMaxAI\n",
      "7 days ago\n",
      "Building\n",
      "on\n",
      "Zero\n",
      "3.57k\n",
      "IC Light V2\n",
      "📈\n",
      "Execute user-defined code\n",
      "lllyasviel\n",
      "Oct 26, 2024\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "340\n",
      "Parakeet-TDT-0.6b-V2\n",
      "Transcribe audio files to text with timestamps\n",
      "nvidia\n",
      "5 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "573\n",
      "ICEdit\n",
      "🖼\n",
      "Universal Image Editing is worth a single LoRA\n",
      "RiverZ\n",
      "13 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "8.33k\n",
      "FLUX.1 [dev]\n",
      "🖥\n",
      "Generate images from text prompts\n",
      "black-forest-labs\n",
      "Apr 16\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "2.67k\n",
      "Hunyuan3D-2.0\n",
      "🌍\n",
      "Text-to-3D and Image-to-3D Generation\n",
      "tencent\n",
      "26 days ago\n",
      "Running\n",
      "86\n",
      "Seed1.5 VL\n",
      "🚀\n",
      "Seed1.5-VL API Demo\n",
      "ByteDance-Seed\n",
      "7 days ago\n",
      "Running\n",
      "on\n",
      "CPU Upgrade\n",
      "10.2k\n",
      "AI Comic Factory\n",
      "👩\n",
      "Create your own AI comic with a single prompt\n",
      "jbilcke-hf\n",
      "Oct 15, 2024\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "1.85k\n",
      "Background Removal\n",
      "🌘\n",
      "Remove background from images\n",
      "not-lain\n",
      "16 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "329\n",
      "NSFW Uncensored image to video\n",
      "🎬\n",
      "MAX ~60sec Video - AI Limits\n",
      "Heartsync\n",
      "2 days ago\n",
      "Running\n",
      "on\n",
      "Zero\n",
      "65\n",
      "DreamO Video\n",
      "🐨\n",
      "A Unified Framework for Custom Image and Video generation\n",
      "openfree\n",
      "1 day ago\n",
      "Previous\n",
      "1\n",
      "2\n",
      "3\n",
      "...\n",
      "100\n",
      "Next\n",
      "System theme\n",
      "Company\n",
      "TOS\n",
      "Privacy\n",
      "About\n",
      "Jobs\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Pricing\n",
      "Docs\n",
      "\n",
      "\n",
      "\n",
      "Documentation\n",
      "Webpage Title:\n",
      "Hugging Face - Documentation\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Documentations\n",
      "Hub & Client Libraries\n",
      "Hub\n",
      "Host Git-based models, datasets, and Spaces on the HF Hub\n",
      "Hub Python Library\n",
      "Python client to interact with the Hugging Face Hub\n",
      "Huggingface.js\n",
      "JavaScript libraries for Hugging Face with built-in TS types\n",
      "Tasks\n",
      "Explore demos, models, and datasets for any ML tasks\n",
      "Dataset viewer\n",
      "API for metadata, stats, and content of HF Hub datasets\n",
      "Deployment & Inference\n",
      "Inference Providers\n",
      "Call 200k+ models hosted by our 10+ Inference partners\n",
      "Inference Endpoints (dedicated)\n",
      "Deploy models on dedicated & fully managed infrastructure on HF\n",
      "Amazon SageMaker\n",
      "Train/deploy Transformers models with SageMaker/HF DLCs\n",
      "Text Generation Inference\n",
      "Serve language models with TGI optimized toolkit\n",
      "Text Embeddings Inference\n",
      "Serve embeddings models with TEI optimized toolkit\n",
      "Core ML Libraries\n",
      "Transformers\n",
      "State-of-the-art ML for PyTorch, TensorFlow, JAX\n",
      "Diffusers\n",
      "State-of-the-art Diffusion models in PyTorch\n",
      "Datasets\n",
      "Access & share datasets for any ML tasks\n",
      "Transformers.js\n",
      "State-of-the-art ML running directly in your browser\n",
      "Tokenizers\n",
      "Fast tokenizers optimized for research & production\n",
      "Evaluate\n",
      "Evaluate and compare models performance\n",
      "timm\n",
      "State-of-the-art vision models: layers, optimizers, and utilities\n",
      "Sentence Transformers\n",
      "Embeddings, Retrieval, and Reranking\n",
      "Training & Optimization\n",
      "PEFT\n",
      "Parameter-efficient finetuning for large language models\n",
      "Accelerate\n",
      "Train PyTorch models with multi-GPU, TPU, mixed precision\n",
      "Optimum\n",
      "Optimize HF Transformers for faster training/inference\n",
      "AWS Trainium & Inferentia\n",
      "Train/deploy Transformers/Diffusers on AWS\n",
      "TRL\n",
      "Train transformers LMs with reinforcement learning\n",
      "Safetensors\n",
      "Safe way to store/distribute neural network weights\n",
      "Bitsandbytes\n",
      "Optimize and quantize models with bitsandbytes\n",
      "Lighteval\n",
      "All-in-one toolkit to evaluate LLMs across multiple backends\n",
      "Collaboration & Extras\n",
      "Gradio\n",
      "Build ML demos and web apps with a few lines of Python\n",
      "smolagents\n",
      "Smol library to build great agents in Python\n",
      "AutoTrain\n",
      "AutoTrain API and UI for seamless model training\n",
      "Chat UI\n",
      "Open source chat frontend powering HuggingChat\n",
      "Leaderboards\n",
      "Create custom Leaderboards on Hugging Face\n",
      "Argilla\n",
      "Collaboration tool for building high-quality datasets\n",
      "Distilabel\n",
      "Framework for synthetic data generation and AI feedback\n",
      "Community\n",
      "Blog\n",
      "Learn\n",
      "Discord\n",
      "Forum\n",
      "Github\n",
      "System theme\n",
      "Company\n",
      "TOS\n",
      "Privacy\n",
      "About\n",
      "Jobs\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Pricing\n",
      "Docs\n",
      "\n",
      "\n",
      "\n",
      "Hub Documentation\n",
      "Webpage Title:\n",
      "🤗 Hub client library\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Hub Python Library documentation\n",
      "🤗 Hub client library\n",
      "Hub Python Library\n",
      "🏡 View all docs\n",
      "AWS Trainium & Inferentia\n",
      "Accelerate\n",
      "Amazon SageMaker\n",
      "Argilla\n",
      "AutoTrain\n",
      "Bitsandbytes\n",
      "Chat UI\n",
      "Dataset viewer\n",
      "Datasets\n",
      "Diffusers\n",
      "Distilabel\n",
      "Evaluate\n",
      "Gradio\n",
      "Hub\n",
      "Hub Python Library\n",
      "Huggingface.js\n",
      "Inference Endpoints (dedicated)\n",
      "Inference Providers\n",
      "Leaderboards\n",
      "Lighteval\n",
      "Optimum\n",
      "PEFT\n",
      "Safetensors\n",
      "Sentence Transformers\n",
      "TRL\n",
      "Tasks\n",
      "Text Embeddings Inference\n",
      "Text Generation Inference\n",
      "Tokenizers\n",
      "Transformers\n",
      "Transformers.js\n",
      "smolagents\n",
      "timm\n",
      "Search documentation\n",
      "main\n",
      "v0.31.4\n",
      "v0.30.2\n",
      "v0.29.3\n",
      "v0.28.1\n",
      "v0.27.1\n",
      "v0.26.5\n",
      "v0.25.2\n",
      "v0.24.7\n",
      "v0.23.5\n",
      "v0.22.2\n",
      "v0.21.4\n",
      "v0.20.3\n",
      "v0.19.3\n",
      "v0.18.0.rc0\n",
      "v0.17.3\n",
      "v0.16.3\n",
      "v0.15.1\n",
      "v0.14.1\n",
      "v0.13.4\n",
      "v0.12.1\n",
      "v0.11.0\n",
      "v0.10.1\n",
      "v0.9.1\n",
      "v0.8.1\n",
      "v0.7.0.rc0\n",
      "v0.6.0.rc0\n",
      "v0.5.1\n",
      "CN\n",
      "DE\n",
      "EN\n",
      "FR\n",
      "HI\n",
      "KO\n",
      "TM\n",
      "Get started\n",
      "Home\n",
      "Quickstart\n",
      "Installation\n",
      "How-to guides\n",
      "Overview\n",
      "Download files\n",
      "Upload files\n",
      "Use the CLI\n",
      "HfFileSystem\n",
      "Repository\n",
      "Search\n",
      "Inference\n",
      "Inference Endpoints\n",
      "Community Tab\n",
      "Collections\n",
      "Cache\n",
      "Model Cards\n",
      "Manage your Space\n",
      "Integrate a library\n",
      "Webhooks\n",
      "Conceptual guides\n",
      "Git vs HTTP paradigm\n",
      "Reference\n",
      "Overview\n",
      "Authentication\n",
      "Environment variables\n",
      "Managing local and online repositories\n",
      "Hugging Face Hub API\n",
      "Downloading files\n",
      "Mixins & serialization methods\n",
      "Inference Types\n",
      "Inference Client\n",
      "Inference Endpoints\n",
      "HfFileSystem\n",
      "Utilities\n",
      "Discussions and Pull Requests\n",
      "Cache-system reference\n",
      "Repo Cards and Repo Card Data\n",
      "Space runtime\n",
      "Collections\n",
      "TensorBoard logger\n",
      "Webhooks server\n",
      "Serialization\n",
      "Strict dataclasses\n",
      "Join the Hugging Face community\n",
      "and get access to the augmented documentation experience\n",
      "Collaborate on models, datasets and Spaces\n",
      "Faster examples with accelerated inference\n",
      "Switch between documentation themes\n",
      "Sign Up\n",
      "to get started\n",
      "🤗 Hub client library\n",
      "The\n",
      "huggingface_hub\n",
      "library allows you to interact with the\n",
      "Hugging Face\n",
      "Hub\n",
      ", a machine learning platform for creators and collaborators.\n",
      "Discover pre-trained models and datasets for your projects or play with the hundreds of\n",
      "machine learning apps hosted on the Hub. You can also create and share your own models\n",
      "and datasets with the community. The\n",
      "huggingface_hub\n",
      "library provides a simple way to\n",
      "do all these things with Python.\n",
      "Read the\n",
      "quick start guide\n",
      "to get up and running with the\n",
      "huggingface_hub\n",
      "library. You will learn how to download files from the Hub, create a\n",
      "repository, and upload files to the Hub. Keep reading to learn more about how to manage\n",
      "your repositories on the 🤗 Hub, how to interact in discussions or even how to access\n",
      "the Inference API.\n",
      "How-to guides\n",
      "Practical guides to help you achieve a specific goal. Take a look at these guides to learn how to use huggingface_hub to solve real-world problems.\n",
      "Reference\n",
      "Exhaustive and technical description of huggingface_hub classes and methods.\n",
      "Conceptual guides\n",
      "High-level explanations for building a better understanding of huggingface_hub philosophy.\n",
      "Contribute\n",
      "All contributions to the\n",
      "huggingface_hub\n",
      "are welcomed and equally valued! 🤗 Besides\n",
      "adding or fixing existing issues in the code, you can also help improve the\n",
      "documentation by making sure it is accurate and up-to-date, help answer questions on\n",
      "issues, and request new features you think will improve the library. Take a look at the\n",
      "contribution\n",
      "guide\n",
      "to\n",
      "learn more about how to submit a new issue or feature request, how to submit a pull\n",
      "request, and how to test your contributions to make sure everything works as expected.\n",
      "Contributors should also be respectful of our\n",
      "code of\n",
      "conduct\n",
      "to\n",
      "create an inclusive and welcoming collaborative space for everyone.\n",
      "<\n",
      ">\n",
      "Update\n",
      "on GitHub\n",
      "Quickstart\n",
      "→\n",
      "🤗\n",
      "Hub client library\n",
      "Contribute\n",
      "\n",
      "\n",
      "\n",
      "Tasks\n",
      "Webpage Title:\n",
      "404 – Hugging Face\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "404\n",
      "Sorry, we can't find the page you are looking for.\n",
      "System theme\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Tasks\n",
      "Inference Endpoints\n",
      "HuggingChat\n",
      "Company\n",
      "About\n",
      "Brand assets\n",
      "Terms of service\n",
      "Privacy\n",
      "Jobs\n",
      "Press\n",
      "Resources\n",
      "Learn\n",
      "Documentation\n",
      "Blog\n",
      "Forum\n",
      "Service Status\n",
      "Social\n",
      "GitHub\n",
      "Twitter\n",
      "LinkedIn\n",
      "Discord\n",
      "\n",
      "\n",
      "\n",
      "Dataset Viewer\n",
      "Webpage Title:\n",
      "🤗 Dataset viewer\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Dataset viewer documentation\n",
      "🤗 Dataset viewer\n",
      "Dataset viewer\n",
      "🏡 View all docs\n",
      "AWS Trainium & Inferentia\n",
      "Accelerate\n",
      "Amazon SageMaker\n",
      "Argilla\n",
      "AutoTrain\n",
      "Bitsandbytes\n",
      "Chat UI\n",
      "Dataset viewer\n",
      "Datasets\n",
      "Diffusers\n",
      "Distilabel\n",
      "Evaluate\n",
      "Gradio\n",
      "Hub\n",
      "Hub Python Library\n",
      "Huggingface.js\n",
      "Inference Endpoints (dedicated)\n",
      "Inference Providers\n",
      "Leaderboards\n",
      "Lighteval\n",
      "Optimum\n",
      "PEFT\n",
      "Safetensors\n",
      "Sentence Transformers\n",
      "TRL\n",
      "Tasks\n",
      "Text Embeddings Inference\n",
      "Text Generation Inference\n",
      "Tokenizers\n",
      "Transformers\n",
      "Transformers.js\n",
      "smolagents\n",
      "timm\n",
      "Search documentation\n",
      "main\n",
      "EN\n",
      "Get Started\n",
      "🤗 Dataset viewer\n",
      "Quickstart\n",
      "Analyze a dataset on the Hub\n",
      "Guides\n",
      "Check dataset validity\n",
      "List splits and subsets\n",
      "Get dataset information\n",
      "Preview a dataset\n",
      "Download slices of rows\n",
      "Search text in a dataset\n",
      "Filter rows in a dataset\n",
      "List Parquet files\n",
      "Get the number of rows and the bytes size\n",
      "Explore dataset statistics\n",
      "Get Croissant metadata\n",
      "Query datasets from dataset viewer API\n",
      "Overview\n",
      "ClickHouse\n",
      "cuDF\n",
      "DuckDB\n",
      "Pandas\n",
      "Polars\n",
      "PostgreSQL\n",
      "mlcroissant\n",
      "PySpark\n",
      "Conceptual Guides\n",
      "Splits and subsets\n",
      "Data types\n",
      "Server infrastructure\n",
      "Join the Hugging Face community\n",
      "and get access to the augmented documentation experience\n",
      "Collaborate on models, datasets and Spaces\n",
      "Faster examples with accelerated inference\n",
      "Switch between documentation themes\n",
      "Sign Up\n",
      "to get started\n",
      "🤗 Dataset viewer\n",
      "The dataset page includes a table with the dataset’s contents, arranged by pages of 100 rows. You can navigate between pages using the buttons at the bottom of the table, filter, search, look at basic statistics, and more.\n",
      "Dataset viewer of the\n",
      "OpenBookQA dataset\n",
      "Contents of the documentation\n",
      "These documentation pages are focused on the\n",
      "dataset viewer’s backend\n",
      "(code in\n",
      "https://github.com/huggingface/dataset-viewer\n",
      "), which provides the table with pre-computed data through an API for all the datasets on the Hub. You can explore the sections if you want to consume the API for your application or to understand how we preprocess the datasets.\n",
      "Otherwise, if you want to learn about creating datasets from the Hub’s web-based interface,\n",
      "configuring the dataset viewer\n",
      "for data,\n",
      "images\n",
      ", or audio, or fixing errors, you might prefer reading the\n",
      "Datasets Hub documentation pages\n",
      ". Take also a look to the\n",
      "example datasets\n",
      "collections:\n",
      "splits configuration\n",
      ",\n",
      "subsets configuration\n",
      ",\n",
      "CSV data files\n",
      "and\n",
      "image datasets\n",
      ".\n",
      "Dataset viewer’s backend\n",
      "The dataset viewer’s backend provides an API for visualizing and exploring all types of datasets - computer vision, speech, text, and tabular - stored on the Hugging Face\n",
      "Hub\n",
      ".\n",
      "The main feature of the dataset viewer’s backend is to auto-convert all the\n",
      "Hub datasets\n",
      "to\n",
      "Parquet\n",
      ". Read more in the\n",
      "Parquet section\n",
      ".\n",
      "As datasets increase in size and data type richness, the cost of preprocessing (storage and compute) these datasets can be challenging and time-consuming.\n",
      "To help users access these modern datasets, The dataset viewer runs a server behind the scenes to generate the API responses ahead of time and stores them in a database so they are instantly returned when you make a query through the API.\n",
      "Let the dataset viewer take care of the heavy lifting so you can use a simple\n",
      "REST API\n",
      "on any of the\n",
      "100,000+ datasets on Hugging Face\n",
      "to:\n",
      "List the\n",
      "dataset splits, column names and data types\n",
      "Get the\n",
      "dataset size\n",
      "(in number of rows or bytes)\n",
      "Download and view\n",
      "rows at any index\n",
      "in the dataset\n",
      "Search\n",
      "a word in the dataset\n",
      "Filter\n",
      "rows based on a query string\n",
      "Get insightful\n",
      "statistics\n",
      "about the data\n",
      "Access the dataset as\n",
      "parquet files\n",
      "to use in your favorite\n",
      "processing or analytics framework\n",
      "Join the growing community on the\n",
      "forum\n",
      "or\n",
      "Discord\n",
      "today, and give the\n",
      "dataset viewer repository\n",
      "a ⭐️ if you’re interested in the latest updates!\n",
      "<\n",
      ">\n",
      "Update\n",
      "on GitHub\n",
      "Quickstart\n",
      "→\n",
      "🤗\n",
      "Dataset viewer\n",
      "Contents of the documentation\n",
      "Dataset viewer’s backend\n",
      "\n",
      "\n",
      "\n",
      "Text Generation Inference\n",
      "Webpage Title:\n",
      "Text Generation Inference\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "text-generation-inference documentation\n",
      "Text Generation Inference\n",
      "text-generation-inference\n",
      "🏡 View all docs\n",
      "AWS Trainium & Inferentia\n",
      "Accelerate\n",
      "Amazon SageMaker\n",
      "Argilla\n",
      "AutoTrain\n",
      "Bitsandbytes\n",
      "Chat UI\n",
      "Dataset viewer\n",
      "Datasets\n",
      "Diffusers\n",
      "Distilabel\n",
      "Evaluate\n",
      "Gradio\n",
      "Hub\n",
      "Hub Python Library\n",
      "Huggingface.js\n",
      "Inference Endpoints (dedicated)\n",
      "Inference Providers\n",
      "Leaderboards\n",
      "Lighteval\n",
      "Optimum\n",
      "PEFT\n",
      "Safetensors\n",
      "Sentence Transformers\n",
      "TRL\n",
      "Tasks\n",
      "Text Embeddings Inference\n",
      "Text Generation Inference\n",
      "Tokenizers\n",
      "Transformers\n",
      "Transformers.js\n",
      "smolagents\n",
      "timm\n",
      "Search documentation\n",
      "main\n",
      "EN\n",
      "Getting started\n",
      "Text Generation Inference\n",
      "Quick Tour\n",
      "Supported Models\n",
      "Using TGI with Nvidia GPUs\n",
      "Using TGI with AMD GPUs\n",
      "Using TGI with Intel Gaudi\n",
      "Using TGI with AWS Trainium and Inferentia\n",
      "Using TGI with Google TPUs\n",
      "Using TGI with Intel GPUs\n",
      "Installation from source\n",
      "Multi-backend support\n",
      "Internal Architecture\n",
      "Usage Statistics\n",
      "Tutorials\n",
      "Consuming TGI\n",
      "Preparing Model for Serving\n",
      "Serving Private & Gated Models\n",
      "Using TGI CLI\n",
      "Non-core Model Serving\n",
      "Safety\n",
      "Using Guidance, JSON, tools\n",
      "Visual Language Models\n",
      "Monitoring TGI with Prometheus and Grafana\n",
      "Train Medusa\n",
      "Backends\n",
      "Neuron\n",
      "Gaudi\n",
      "TensorRT-LLM\n",
      "Llamacpp\n",
      "Reference\n",
      "All TGI CLI options\n",
      "Exported Metrics\n",
      "API Reference\n",
      "Conceptual Guides\n",
      "V3 update, caching and chunking\n",
      "Streaming\n",
      "Quantization\n",
      "Tensor Parallelism\n",
      "PagedAttention\n",
      "Safetensors\n",
      "Flash Attention\n",
      "Speculation (Medusa, ngram)\n",
      "How Guidance Works (via outlines)\n",
      "LoRA (Low-Rank Adaptation)\n",
      "External Resources\n",
      "Join the Hugging Face community\n",
      "and get access to the augmented documentation experience\n",
      "Collaborate on models, datasets and Spaces\n",
      "Faster examples with accelerated inference\n",
      "Switch between documentation themes\n",
      "Sign Up\n",
      "to get started\n",
      "Text Generation Inference\n",
      "Text Generation Inference (TGI) is a toolkit for deploying and serving Large Language Models (LLMs). TGI enables high-performance text generation for the most popular open-source LLMs, including Llama, Falcon, StarCoder, BLOOM, GPT-NeoX, and T5.\n",
      "Text Generation Inference implements many optimizations and features, such as:\n",
      "Simple launcher to serve most popular LLMs\n",
      "Production ready (distributed tracing with Open Telemetry, Prometheus metrics)\n",
      "Tensor Parallelism for faster inference on multiple GPUs\n",
      "Token streaming using Server-Sent Events (SSE)\n",
      "Continuous batching of incoming requests for increased total throughput\n",
      "Optimized transformers code for inference using\n",
      "Flash Attention\n",
      "and\n",
      "Paged Attention\n",
      "on the most popular architectures\n",
      "Quantization with\n",
      "bitsandbytes\n",
      "and\n",
      "GPT-Q\n",
      "Safetensors\n",
      "weight loading\n",
      "Watermarking with\n",
      "A Watermark for Large Language Models\n",
      "Logits warper (temperature scaling, top-p, top-k, repetition penalty)\n",
      "Stop sequences\n",
      "Log probabilities\n",
      "Fine-tuning Support: Utilize fine-tuned models for specific tasks to achieve higher accuracy and performance.\n",
      "Guidance\n",
      ": Enable function calling and tool-use by forcing the model to generate structured outputs based on your own predefined output schemas.\n",
      "Text Generation Inference is used in production by multiple projects, such as:\n",
      "Hugging Chat\n",
      ", an open-source interface for open-access models, such as Open Assistant and Llama\n",
      "OpenAssistant\n",
      ", an open-source community effort to train LLMs in the open\n",
      "nat.dev\n",
      ", a playground to explore and compare LLMs.\n",
      "<\n",
      ">\n",
      "Update\n",
      "on GitHub\n",
      "Quick Tour\n",
      "→\n",
      "Text\n",
      "Generation\n",
      "Inference\n",
      "\n",
      "\n",
      "\n",
      "Transformers\n",
      "Webpage Title:\n",
      "Transformers\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Transformers documentation\n",
      "Transformers\n",
      "Transformers\n",
      "🏡 View all docs\n",
      "AWS Trainium & Inferentia\n",
      "Accelerate\n",
      "Amazon SageMaker\n",
      "Argilla\n",
      "AutoTrain\n",
      "Bitsandbytes\n",
      "Chat UI\n",
      "Dataset viewer\n",
      "Datasets\n",
      "Diffusers\n",
      "Distilabel\n",
      "Evaluate\n",
      "Gradio\n",
      "Hub\n",
      "Hub Python Library\n",
      "Huggingface.js\n",
      "Inference Endpoints (dedicated)\n",
      "Inference Providers\n",
      "Leaderboards\n",
      "Lighteval\n",
      "Optimum\n",
      "PEFT\n",
      "Safetensors\n",
      "Sentence Transformers\n",
      "TRL\n",
      "Tasks\n",
      "Text Embeddings Inference\n",
      "Text Generation Inference\n",
      "Tokenizers\n",
      "Transformers\n",
      "Transformers.js\n",
      "smolagents\n",
      "timm\n",
      "Search documentation\n",
      "main\n",
      "v4.52.1\n",
      "v4.51.3\n",
      "v4.50.0\n",
      "v4.49.0\n",
      "v4.48.2\n",
      "v4.47.1\n",
      "v4.46.3\n",
      "v4.45.2\n",
      "v4.44.2\n",
      "v4.43.4\n",
      "v4.42.4\n",
      "v4.41.2\n",
      "v4.40.2\n",
      "v4.39.3\n",
      "v4.38.2\n",
      "v4.37.2\n",
      "v4.36.1\n",
      "v4.35.2\n",
      "v4.34.1\n",
      "v4.33.3\n",
      "v4.32.1\n",
      "v4.31.0\n",
      "v4.30.0\n",
      "v4.29.1\n",
      "v4.28.1\n",
      "v4.27.2\n",
      "v4.26.1\n",
      "v4.25.1\n",
      "v4.24.0\n",
      "v4.23.1\n",
      "v4.22.2\n",
      "v4.21.3\n",
      "v4.20.1\n",
      "v4.19.4\n",
      "v4.18.0\n",
      "v4.17.0\n",
      "v4.16.2\n",
      "v4.15.0\n",
      "v4.14.1\n",
      "v4.13.0\n",
      "v4.12.5\n",
      "v4.11.3\n",
      "v4.10.1\n",
      "v4.9.2\n",
      "v4.8.2\n",
      "v4.7.0\n",
      "v4.6.0\n",
      "v4.5.1\n",
      "v4.4.2\n",
      "v4.3.3\n",
      "v4.2.2\n",
      "v4.1.1\n",
      "v4.0.1\n",
      "v3.5.1\n",
      "v3.4.0\n",
      "v3.3.1\n",
      "v3.2.0\n",
      "v3.1.0\n",
      "v3.0.2\n",
      "v2.11.0\n",
      "v2.10.0\n",
      "v2.9.1\n",
      "v2.8.0\n",
      "v2.7.0\n",
      "v2.6.0\n",
      "v2.5.1\n",
      "v2.4.1\n",
      "v2.3.0\n",
      "v2.2.2\n",
      "v2.1.1\n",
      "v2.0.0\n",
      "v1.2.0\n",
      "v1.1.0\n",
      "v1.0.0\n",
      "doc-builder-html\n",
      "AR\n",
      "DE\n",
      "EN\n",
      "ES\n",
      "FR\n",
      "HI\n",
      "IT\n",
      "JA\n",
      "KO\n",
      "PT\n",
      "TE\n",
      "TR\n",
      "ZH\n",
      "Get started\n",
      "Transformers\n",
      "Installation\n",
      "Quickstart\n",
      "Base classes\n",
      "Inference\n",
      "Training\n",
      "Quantization\n",
      "Export to production\n",
      "Resources\n",
      "Contribute\n",
      "API\n",
      "Join the Hugging Face community\n",
      "and get access to the augmented documentation experience\n",
      "Collaborate on models, datasets and Spaces\n",
      "Faster examples with accelerated inference\n",
      "Switch between documentation themes\n",
      "Sign Up\n",
      "to get started\n",
      "Transformers\n",
      "Transformers is a library of pretrained natural language processing, computer vision, audio, and multimodal models for inference and training. Use Transformers to train models on your data, build inference applications, and generate text with large language models.\n",
      "Explore the\n",
      "Hugging Face Hub\n",
      "today to find a model and use Transformers to help you get started right away.\n",
      "Features\n",
      "Transformers provides everything you need for inference or training with state-of-the-art pretrained models. Some of the main features include:\n",
      "Pipeline\n",
      ": Simple and optimized inference class for many machine learning tasks like text generation, image segmentation, automatic speech recognition, document question answering, and more.\n",
      "Trainer\n",
      ": A comprehensive trainer that supports features such as mixed precision, torch.compile, and FlashAttention for training and distributed training for PyTorch models.\n",
      "generate\n",
      ": Fast text generation with large language models (LLMs) and vision language models (VLMs), including support for streaming and multiple decoding strategies.\n",
      "Design\n",
      "Read our\n",
      "Philosophy\n",
      "to learn more about Transformers’ design principles.\n",
      "Transformers is designed for developers and machine learning engineers and researchers. Its main design principles are:\n",
      "Fast and easy to use: Every model is implemented from only three main classes (configuration, model, and preprocessor) and can be quickly used for inference or training with\n",
      "Pipeline\n",
      "or\n",
      "Trainer\n",
      ".\n",
      "Pretrained models: Reduce your carbon footprint, compute cost and time by using a pretrained model instead of training an entirely new one. Each pretrained model is reproduced as closely as possible to the original model and offers state-of-the-art performance.\n",
      "<\n",
      ">\n",
      "Update\n",
      "on GitHub\n",
      "Installation\n",
      "→\n",
      "Transformers\n",
      "Features\n",
      "Design\n",
      "\n",
      "\n",
      "\n",
      "Diffusers\n",
      "Webpage Title:\n",
      "Diffusers\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Diffusers documentation\n",
      "Diffusers\n",
      "Diffusers\n",
      "🏡 View all docs\n",
      "AWS Trainium & Inferentia\n",
      "Accelerate\n",
      "Amazon SageMaker\n",
      "Argilla\n",
      "AutoTrain\n",
      "Bitsandbytes\n",
      "Chat UI\n",
      "Dataset viewer\n",
      "Datasets\n",
      "Diffusers\n",
      "Distilabel\n",
      "Evaluate\n",
      "Gradio\n",
      "Hub\n",
      "Hub Python Library\n",
      "Huggingface.js\n",
      "Inference Endpoints (dedicated)\n",
      "Inference Providers\n",
      "Leaderboards\n",
      "Lighteval\n",
      "Optimum\n",
      "PEFT\n",
      "Safetensors\n",
      "Sentence Transformers\n",
      "TRL\n",
      "Tasks\n",
      "Text Embeddings Inference\n",
      "Text Generation Inference\n",
      "Tokenizers\n",
      "Transformers\n",
      "Transformers.js\n",
      "smolagents\n",
      "timm\n",
      "Search documentation\n",
      "main\n",
      "v0.33.1\n",
      "v0.32.2\n",
      "v0.31.0\n",
      "v0.30.3\n",
      "v0.29.2\n",
      "v0.28.2\n",
      "v0.27.2\n",
      "v0.26.3\n",
      "v0.25.1\n",
      "v0.24.0\n",
      "v0.23.1\n",
      "v0.22.3\n",
      "v0.21.0\n",
      "v0.20.0\n",
      "v0.19.3\n",
      "v0.18.2\n",
      "v0.17.1\n",
      "v0.16.0\n",
      "v0.15.0\n",
      "v0.14.0\n",
      "v0.13.0\n",
      "v0.12.0\n",
      "v0.11.0\n",
      "v0.10.2\n",
      "v0.9.0\n",
      "v0.8.0\n",
      "v0.7.0\n",
      "v0.6.0\n",
      "v0.5.1\n",
      "v0.4.1\n",
      "v0.3.0\n",
      "v0.2.4\n",
      "EN\n",
      "JA\n",
      "KO\n",
      "PT\n",
      "ZH\n",
      "Get started\n",
      "🧨 Diffusers\n",
      "Quicktour\n",
      "Effective and efficient diffusion\n",
      "Installation\n",
      "Tutorials\n",
      "Overview\n",
      "Understanding pipelines, models and schedulers\n",
      "AutoPipeline\n",
      "Train a diffusion model\n",
      "Load LoRAs for inference\n",
      "Accelerate inference of text-to-image diffusion models\n",
      "Working with big models\n",
      "Load pipelines and adapters\n",
      "Load pipelines\n",
      "Load community pipelines and components\n",
      "Load schedulers and models\n",
      "Model files and layouts\n",
      "Load adapters\n",
      "Push files to the Hub\n",
      "Generative tasks\n",
      "Unconditional image generation\n",
      "Text-to-image\n",
      "Image-to-image\n",
      "Inpainting\n",
      "Video generation\n",
      "Depth-to-image\n",
      "Inference techniques\n",
      "Overview\n",
      "Create a server\n",
      "Distributed inference\n",
      "Merge LoRAs\n",
      "Scheduler features\n",
      "Pipeline callbacks\n",
      "Reproducible pipelines\n",
      "Controlling image quality\n",
      "Prompt techniques\n",
      "Advanced inference\n",
      "Outpainting\n",
      "Hybrid Inference\n",
      "Overview\n",
      "VAE Decode\n",
      "VAE Encode\n",
      "API Reference\n",
      "Specific pipeline examples\n",
      "CogVideoX\n",
      "ConsisID\n",
      "Stable Diffusion XL\n",
      "SDXL Turbo\n",
      "Kandinsky\n",
      "IP-Adapter\n",
      "OmniGen\n",
      "PAG\n",
      "ControlNet\n",
      "T2I-Adapter\n",
      "Latent Consistency Model\n",
      "Textual inversion\n",
      "Shap-E\n",
      "DiffEdit\n",
      "Trajectory Consistency Distillation-LoRA\n",
      "Stable Video Diffusion\n",
      "Marigold Computer Vision\n",
      "Training\n",
      "Overview\n",
      "Create a dataset for training\n",
      "Adapt a model to a new task\n",
      "Models\n",
      "Methods\n",
      "Quantization Methods\n",
      "Getting Started\n",
      "bitsandbytes\n",
      "gguf\n",
      "torchao\n",
      "quanto\n",
      "Accelerate inference and reduce memory\n",
      "Speed up inference\n",
      "Reduce memory usage\n",
      "PyTorch 2.0\n",
      "xFormers\n",
      "Token merging\n",
      "DeepCache\n",
      "TGATE\n",
      "xDiT\n",
      "ParaAttention\n",
      "Optimized model formats\n",
      "JAX/Flax\n",
      "ONNX\n",
      "OpenVINO\n",
      "Core ML\n",
      "Optimized hardware\n",
      "Metal Performance Shaders (MPS)\n",
      "Habana Gaudi\n",
      "AWS Neuron\n",
      "Conceptual Guides\n",
      "Philosophy\n",
      "Controlled generation\n",
      "How to contribute?\n",
      "Diffusers' Ethical Guidelines\n",
      "Evaluating Diffusion Models\n",
      "Community Projects\n",
      "Projects built with Diffusers\n",
      "API\n",
      "Main Classes\n",
      "Loaders\n",
      "Models\n",
      "Pipelines\n",
      "Schedulers\n",
      "Internal classes\n",
      "Join the Hugging Face community\n",
      "and get access to the augmented documentation experience\n",
      "Collaborate on models, datasets and Spaces\n",
      "Faster examples with accelerated inference\n",
      "Switch between documentation themes\n",
      "Sign Up\n",
      "to get started\n",
      "Diffusers\n",
      "🤗 Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. Whether you’re looking for a simple inference solution or want to train your own diffusion model, 🤗 Diffusers is a modular toolbox that supports both. Our library is designed with a focus on\n",
      "usability over performance\n",
      ",\n",
      "simple over easy\n",
      ", and\n",
      "customizability over abstractions\n",
      ".\n",
      "The library has three main components:\n",
      "State-of-the-art diffusion pipelines for inference with just a few lines of code. There are many pipelines in 🤗 Diffusers, check out the table in the pipeline\n",
      "overview\n",
      "for a complete list of available pipelines and the task they solve.\n",
      "Interchangeable\n",
      "noise schedulers\n",
      "for balancing trade-offs between generation speed and quality.\n",
      "Pretrained\n",
      "models\n",
      "that can be used as building blocks, and combined with schedulers, for creating your own end-to-end diffusion systems.\n",
      "Tutorials\n",
      "Learn the fundamental skills you need to start generating outputs, build your own diffusion system, and train a diffusion model. We recommend starting here if you're using 🤗 Diffusers for the first time!\n",
      "How-to guides\n",
      "Practical guides for helping you load pipelines, models, and schedulers. You'll also learn how to use pipelines for specific tasks, control how outputs are generated, optimize for inference speed, and different training techniques.\n",
      "Conceptual guides\n",
      "Understand why the library was designed the way it was, and learn more about the ethical guidelines and safety implementations for using the library.\n",
      "Reference\n",
      "Technical descriptions of how 🤗 Diffusers classes and methods work.\n",
      "<\n",
      ">\n",
      "Update\n",
      "on GitHub\n",
      "Quicktour\n",
      "→\n",
      "Diffusers\n",
      "\n",
      "\n",
      "\n",
      "Tokenizers\n",
      "Webpage Title:\n",
      "Tokenizers\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Tokenizers documentation\n",
      "Tokenizers\n",
      "Tokenizers\n",
      "🏡 View all docs\n",
      "AWS Trainium & Inferentia\n",
      "Accelerate\n",
      "Amazon SageMaker\n",
      "Argilla\n",
      "AutoTrain\n",
      "Bitsandbytes\n",
      "Chat UI\n",
      "Dataset viewer\n",
      "Datasets\n",
      "Diffusers\n",
      "Distilabel\n",
      "Evaluate\n",
      "Gradio\n",
      "Hub\n",
      "Hub Python Library\n",
      "Huggingface.js\n",
      "Inference Endpoints (dedicated)\n",
      "Inference Providers\n",
      "Leaderboards\n",
      "Lighteval\n",
      "Optimum\n",
      "PEFT\n",
      "Safetensors\n",
      "Sentence Transformers\n",
      "TRL\n",
      "Tasks\n",
      "Text Embeddings Inference\n",
      "Text Generation Inference\n",
      "Tokenizers\n",
      "Transformers\n",
      "Transformers.js\n",
      "smolagents\n",
      "timm\n",
      "Search documentation\n",
      "main\n",
      "v0.20.3\n",
      "v0.13.4.rc2\n",
      "v0.10.0\n",
      "v0.9.4\n",
      "EN\n",
      "Getting started\n",
      "🤗 Tokenizers\n",
      "Quicktour\n",
      "Installation\n",
      "The tokenization pipeline\n",
      "Components\n",
      "Training from memory\n",
      "API\n",
      "Input Sequences\n",
      "Encode Inputs\n",
      "Tokenizer\n",
      "Encoding\n",
      "Added Tokens\n",
      "Models\n",
      "Normalizers\n",
      "Pre-tokenizers\n",
      "Post-processors\n",
      "Trainers\n",
      "Decoders\n",
      "Visualizer\n",
      "Join the Hugging Face community\n",
      "and get access to the augmented documentation experience\n",
      "Collaborate on models, datasets and Spaces\n",
      "Faster examples with accelerated inference\n",
      "Switch between documentation themes\n",
      "Sign Up\n",
      "to get started\n",
      "Tokenizers\n",
      "Fast State-of-the-art tokenizers, optimized for both research and\n",
      "production\n",
      "🤗 Tokenizers\n",
      "provides an\n",
      "implementation of today’s most used tokenizers, with a focus on\n",
      "performance and versatility. These tokenizers are also used in\n",
      "🤗 Transformers\n",
      ".\n",
      "Main features:\n",
      "Train new vocabularies and tokenize, using today’s most used tokenizers.\n",
      "Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes less than 20 seconds to tokenize a GB of text on a server’s CPU.\n",
      "Easy to use, but also extremely versatile.\n",
      "Designed for both research and production.\n",
      "Full alignment tracking. Even with destructive normalization, it’s always possible to get the part of the original sentence that corresponds to any token.\n",
      "Does all the pre-processing: Truncation, Padding, add the special tokens your model needs.\n",
      "<\n",
      ">\n",
      "Update\n",
      "on GitHub\n",
      "Quicktour\n",
      "→\n",
      "Tokenizers\n",
      "\n",
      "\n",
      "\n",
      "Evaluate\n",
      "Webpage Title:\n",
      "🤗 Evaluate\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Evaluate documentation\n",
      "🤗 Evaluate\n",
      "Evaluate\n",
      "🏡 View all docs\n",
      "AWS Trainium & Inferentia\n",
      "Accelerate\n",
      "Amazon SageMaker\n",
      "Argilla\n",
      "AutoTrain\n",
      "Bitsandbytes\n",
      "Chat UI\n",
      "Dataset viewer\n",
      "Datasets\n",
      "Diffusers\n",
      "Distilabel\n",
      "Evaluate\n",
      "Gradio\n",
      "Hub\n",
      "Hub Python Library\n",
      "Huggingface.js\n",
      "Inference Endpoints (dedicated)\n",
      "Inference Providers\n",
      "Leaderboards\n",
      "Lighteval\n",
      "Optimum\n",
      "PEFT\n",
      "Safetensors\n",
      "Sentence Transformers\n",
      "TRL\n",
      "Tasks\n",
      "Text Embeddings Inference\n",
      "Text Generation Inference\n",
      "Tokenizers\n",
      "Transformers\n",
      "Transformers.js\n",
      "smolagents\n",
      "timm\n",
      "Search documentation\n",
      "main\n",
      "v0.4.0\n",
      "v0.3.0\n",
      "v0.2.3\n",
      "v0.1.2\n",
      "EN\n",
      "Get started\n",
      "🤗 Evaluate\n",
      "Tutorials\n",
      "Installation\n",
      "A quick tour\n",
      "How-to guides\n",
      "Choosing the right metric\n",
      "Adding new evaluations\n",
      "Using the evaluator\n",
      "Using the evaluator with custom pipelines\n",
      "Creating an EvaluationSuite\n",
      "Using 🤗 Evaluate with other ML frameworks\n",
      "Transformers\n",
      "Keras and Tensorflow\n",
      "scikit-learn\n",
      "Conceptual guides\n",
      "Types of evaluations\n",
      "Considerations for model evaluation\n",
      "Reference\n",
      "Main classes\n",
      "Loading methods\n",
      "Saving methods\n",
      "Hub methods\n",
      "Evaluator classes\n",
      "Visualization methods\n",
      "Logging methods\n",
      "Join the Hugging Face community\n",
      "and get access to the augmented documentation experience\n",
      "Collaborate on models, datasets and Spaces\n",
      "Faster examples with accelerated inference\n",
      "Switch between documentation themes\n",
      "Sign Up\n",
      "to get started\n",
      "🤗 Evaluate\n",
      "A library for easily evaluating machine learning models and datasets.\n",
      "With a single line of code, you get access to dozens of evaluation methods for different domains (NLP, Computer Vision, Reinforcement Learning, and more!). Be it on your local machine or in a distributed training setup, you can evaluate your models in a consistent and reproducible way!\n",
      "Visit the 🤗 Evaluate\n",
      "organization\n",
      "for a full list of available metrics. Each metric has a dedicated Space with an interactive demo for how to use the metric, and a documentation card detailing the metrics limitations and usage.\n",
      "Tutorials\n",
      "Learn the basics and become familiar with loading, computing, and saving with 🤗 Evaluate. Start here if you are using 🤗 Evaluate for the first time!\n",
      "How-to guides\n",
      "Practical guides to help you achieve a specific goal. Take a look at these guides to learn how to use 🤗 Evaluate to solve real-world problems.\n",
      "Conceptual guides\n",
      "High-level explanations for building a better understanding of important topics such as considerations going into evaluating a model or dataset and the difference between metrics, measurements, and comparisons.\n",
      "Reference\n",
      "Technical descriptions of how 🤗 Evaluate classes and methods work.\n",
      "Installation\n",
      "→\n",
      "🤗\n",
      "Evaluate\n",
      "\n",
      "\n",
      "\n",
      "Accelerate\n",
      "Webpage Title:\n",
      "Accelerate\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Accelerate documentation\n",
      "Accelerate\n",
      "Accelerate\n",
      "🏡 View all docs\n",
      "AWS Trainium & Inferentia\n",
      "Accelerate\n",
      "Amazon SageMaker\n",
      "Argilla\n",
      "AutoTrain\n",
      "Bitsandbytes\n",
      "Chat UI\n",
      "Dataset viewer\n",
      "Datasets\n",
      "Diffusers\n",
      "Distilabel\n",
      "Evaluate\n",
      "Gradio\n",
      "Hub\n",
      "Hub Python Library\n",
      "Huggingface.js\n",
      "Inference Endpoints (dedicated)\n",
      "Inference Providers\n",
      "Leaderboards\n",
      "Lighteval\n",
      "Optimum\n",
      "PEFT\n",
      "Safetensors\n",
      "Sentence Transformers\n",
      "TRL\n",
      "Tasks\n",
      "Text Embeddings Inference\n",
      "Text Generation Inference\n",
      "Tokenizers\n",
      "Transformers\n",
      "Transformers.js\n",
      "smolagents\n",
      "timm\n",
      "Search documentation\n",
      "main\n",
      "v1.7.0\n",
      "v1.6.0\n",
      "v1.5.2\n",
      "v1.4.0\n",
      "v1.3.0\n",
      "v1.2.1\n",
      "v1.1.0\n",
      "v1.0.1\n",
      "v0.34.2\n",
      "v0.33.0\n",
      "v0.32.0\n",
      "v0.31.0\n",
      "v0.30.1\n",
      "v0.29.3\n",
      "v0.28.0\n",
      "v0.27.2\n",
      "v0.26.1\n",
      "v0.25.0\n",
      "v0.24.0\n",
      "v0.23.0\n",
      "v0.22.0\n",
      "v0.21.0\n",
      "v0.20.3\n",
      "v0.19.0\n",
      "v0.18.0\n",
      "v0.17.1\n",
      "v0.16.0\n",
      "v0.15.0\n",
      "v0.14.0\n",
      "v0.13.2\n",
      "v0.12.0\n",
      "v0.11.0\n",
      "v0.10.0\n",
      "v0.9.0\n",
      "v0.8.0\n",
      "v0.7.1\n",
      "v0.6.0\n",
      "v0.5.1\n",
      "v0.4.0\n",
      "v0.3.0\n",
      "v0.2.1\n",
      "v0.1.0\n",
      "EN\n",
      "Getting started\n",
      "🤗 Accelerate\n",
      "Installation\n",
      "Quicktour\n",
      "Tutorials\n",
      "Overview\n",
      "Add Accelerate to your code\n",
      "Execution process\n",
      "TPU training\n",
      "Launching Accelerate scripts\n",
      "Launching distributed training from Jupyter Notebooks\n",
      "How to guides\n",
      "Accelerate\n",
      "Start Here!\n",
      "Model memory estimator\n",
      "Model quantization\n",
      "Experiment trackers\n",
      "Profiler\n",
      "Checkpointing\n",
      "Troubleshoot\n",
      "Example Zoo\n",
      "Training\n",
      "Gradient accumulation\n",
      "Local SGD\n",
      "Low precision (FP8) training\n",
      "DeepSpeed\n",
      "Using multiple models with DeepSpeed\n",
      "DDP Communication Hooks\n",
      "Fully Sharded Data Parallel\n",
      "Megatron-LM\n",
      "Amazon SageMaker\n",
      "Apple M1 GPUs\n",
      "IPEX training with CPU\n",
      "Intel Gaudi\n",
      "Compilation\n",
      "Inference\n",
      "Big Model Inference\n",
      "Distributed inference\n",
      "Concepts and fundamentals\n",
      "Accelerate's internal mechanism\n",
      "Loading big models into memory\n",
      "Comparing performance across distributed setups\n",
      "Executing and deferring jobs\n",
      "Gradient synchronization\n",
      "FSDP vs DeepSpeed\n",
      "FSDP1 vs FSDP2\n",
      "Low precision training methods\n",
      "Training on TPUs\n",
      "Reference\n",
      "Accelerator\n",
      "Stateful classes\n",
      "The Command Line\n",
      "DataLoaders, Optimizers, Schedulers\n",
      "Experiment trackers\n",
      "Launchers\n",
      "DeepSpeed utilities\n",
      "Logging\n",
      "Working with large models\n",
      "Pipeline parallelism\n",
      "Kwargs handlers\n",
      "FP8\n",
      "Utility functions and classes\n",
      "Megatron-LM utilities\n",
      "Fully Sharded Data Parallel utilities\n",
      "Join the Hugging Face community\n",
      "and get access to the augmented documentation experience\n",
      "Collaborate on models, datasets and Spaces\n",
      "Faster examples with accelerated inference\n",
      "Switch between documentation themes\n",
      "Sign Up\n",
      "to get started\n",
      "Accelerate\n",
      "Accelerate is a library that enables the same PyTorch code to be run across any distributed configuration by adding just four lines of code! In short, training and inference at scale made simple, efficient and adaptable.\n",
      "Copied\n",
      "+ from accelerate import Accelerator\n",
      "+ accelerator = Accelerator()\n",
      "+ model, optimizer, training_dataloader, scheduler = accelerator.prepare(\n",
      "+     model, optimizer, training_dataloader, scheduler\n",
      "+ )\n",
      "for batch in training_dataloader:\n",
      "      optimizer.zero_grad()\n",
      "      inputs, targets = batch\n",
      "      inputs = inputs.to(device)\n",
      "      targets = targets.to(device)\n",
      "      outputs = model(inputs)\n",
      "      loss = loss_function(outputs, targets)\n",
      "+     accelerator.backward(loss)\n",
      "optimizer.step()\n",
      "      scheduler.step()\n",
      "Built on\n",
      "torch_xla\n",
      "and\n",
      "torch.distributed\n",
      ", Accelerate takes care of the heavy lifting, so you don’t have to write any custom code to adapt to these platforms.\n",
      "Convert existing codebases to utilize\n",
      "DeepSpeed\n",
      ", perform\n",
      "fully sharded data parallelism\n",
      ", and have automatic support for mixed-precision training!\n",
      "To get a better idea of this process, make sure to check out the\n",
      "Tutorials\n",
      "!\n",
      "This code can then be launched on any system through Accelerate’s CLI interface:\n",
      "Copied\n",
      "accelerate launch {my_script.py}\n",
      "Tutorials\n",
      "Learn the basics and become familiar with using Accelerate. Start here if you are using Accelerate for the first time!\n",
      "How-to guides\n",
      "Practical guides to help you achieve a specific goal. Take a look at these guides to learn how to use Accelerate to solve real-world problems.\n",
      "Conceptual guides\n",
      "High-level explanations for building a better understanding of important topics such as avoiding subtle nuances and pitfalls in distributed training and DeepSpeed.\n",
      "Reference\n",
      "Technical descriptions of how Accelerate classes and methods work.\n",
      "<\n",
      ">\n",
      "Update\n",
      "on GitHub\n",
      "Installation\n",
      "→\n",
      "Accelerate\n",
      "\n",
      "\n",
      "\n",
      "Autotrain\n",
      "Webpage Title:\n",
      "AutoTrain\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "AutoTrain documentation\n",
      "AutoTrain\n",
      "AutoTrain\n",
      "🏡 View all docs\n",
      "AWS Trainium & Inferentia\n",
      "Accelerate\n",
      "Amazon SageMaker\n",
      "Argilla\n",
      "AutoTrain\n",
      "Bitsandbytes\n",
      "Chat UI\n",
      "Dataset viewer\n",
      "Datasets\n",
      "Diffusers\n",
      "Distilabel\n",
      "Evaluate\n",
      "Gradio\n",
      "Hub\n",
      "Hub Python Library\n",
      "Huggingface.js\n",
      "Inference Endpoints (dedicated)\n",
      "Inference Providers\n",
      "Leaderboards\n",
      "Lighteval\n",
      "Optimum\n",
      "PEFT\n",
      "Safetensors\n",
      "Sentence Transformers\n",
      "TRL\n",
      "Tasks\n",
      "Text Embeddings Inference\n",
      "Text Generation Inference\n",
      "Tokenizers\n",
      "Transformers\n",
      "Transformers.js\n",
      "smolagents\n",
      "timm\n",
      "Search documentation\n",
      "main\n",
      "v0.8.24\n",
      "v0.7.129\n",
      "v0.6.48\n",
      "v0.5.2\n",
      "EN\n",
      "Getting Started\n",
      "🤗 AutoTrain\n",
      "How much does it cost?\n",
      "Get help and support\n",
      "Frequently Asked Questions\n",
      "Quickstart\n",
      "Train on Spaces\n",
      "Python SDK\n",
      "Train Locally\n",
      "Config File\n",
      "Tasks\n",
      "LLM Finetuning\n",
      "Text Classification/Regression\n",
      "Extractive QA\n",
      "Sentence Transformer\n",
      "Image Classification / Regression\n",
      "Object Detection\n",
      "Seq2Seq\n",
      "Token Classification\n",
      "Tabular\n",
      "Miscellaneous\n",
      "Understanding Column Mapping\n",
      "AutoTrain API\n",
      "You are viewing\n",
      "main\n",
      "version, which requires\n",
      "installation from source\n",
      ". If you'd like\n",
      "\t\t\tregular pip install, checkout the latest stable version (\n",
      "v0.8.24\n",
      ").\n",
      "Join the Hugging Face community\n",
      "and get access to the augmented documentation experience\n",
      "Collaborate on models, datasets and Spaces\n",
      "Faster examples with accelerated inference\n",
      "Switch between documentation themes\n",
      "Sign Up\n",
      "to get started\n",
      "AutoTrain\n",
      "🤗 AutoTrain Advanced (or simply AutoTrain), developed by Hugging Face, is a robust no-code\n",
      "platform designed to simplify the process of training state-of-the-art models across\n",
      "multiple domains: Natural Language Processing (NLP), Computer Vision (CV),\n",
      "and even Tabular Data analysis. This tool leverages the powerful frameworks created by\n",
      "various teams at Hugging Face, making advanced machine learning and artificial intelligence accessible to a broader\n",
      "audience without requiring deep technical expertise.\n",
      "Who should use AutoTrain?\n",
      "AutoTrain is the perfect tool for anyone eager to dive into the world of machine learning\n",
      "without getting bogged down by the complexities of model training.\n",
      "Whether you’re a business professional, researcher, educator, or hobbyist,\n",
      "AutoTrain offers the simplicity of a no-code interface while still providing the\n",
      "capabilities necessary to develop sophisticated models tailored to your unique datasets.\n",
      "AutoTrain is for anyone who wants to train a state-of-the-art model for a NLP, CV, Speech or even Tabular task,\n",
      "but doesn’t want to spend time on the technical details of training a model.\n",
      "Our mission is to democratize machine learning technology, ensuring it is not only\n",
      "accessible to data scientists and ML engineers but also to those without a technical\n",
      "background. If you’re looking to harness the power of AI for your projects,\n",
      "AutoTrain is your answer.\n",
      "How to use AutoTrain?\n",
      "We offer several ways to use AutoTrain:\n",
      "No code users can use\n",
      "AutoTrain Advanced\n",
      "by creating a new space with AutoTrain Docker image:\n",
      "Click here\n",
      "to create AutoTrain Space.\n",
      "Remember to keep your space private and ensure it is equipped with the necessary hardware resources (GPU) for optimal performance.\n",
      "If you prefer a more hands-on approach, AutoTrain Advanced can also be run locally\n",
      "through its intuitive UI or accessed via the Python API provided in the autotrain-advanced\n",
      "package. This flexibility allows developers to integrate AutoTrain capabilities directly\n",
      "into their projects, customize workflows, and enhance their toolsets with advanced machine\n",
      "learning functionalities.\n",
      "By bridging the gap between cutting-edge technology and practical usability,\n",
      "AutoTrain Advanced empowers users to achieve remarkable results in AI without the need\n",
      "for extensive programming knowledge. Start your journey with AutoTrain today and unlock\n",
      "the potential of machine learning for your projects!\n",
      "Walkthroughs\n",
      "To get started with AutoTrain, check out our walkthroughs and tutorials:\n",
      "Extractive Question Answering with AutoTrain\n",
      "Finetuning PaliGemma with AutoTrain\n",
      "Training an Object Detection Model with AutoTrain\n",
      "How to Fine-Tune Custom Embedding Models Using AutoTrain\n",
      "Train Custom Models on Hugging Face Spaces with AutoTrain SpaceRunner\n",
      "How to Finetune phi-3 on MacBook Pro\n",
      "Finetune Mixtral 8x7B with AutoTrain\n",
      "Easily Train Models with H100 GPUs on NVIDIA DGX Cloud\n",
      "<\n",
      ">\n",
      "Update\n",
      "on GitHub\n",
      "How much does it cost?\n",
      "→\n",
      "Auto\n",
      "Train\n",
      "Who should use\n",
      "Auto\n",
      "Train?\n",
      "How to use\n",
      "Auto\n",
      "Train?\n",
      "Walkthroughs\n",
      "\n",
      "\n",
      "\n",
      "Chat UI\n",
      "Webpage Title:\n",
      "🤗 Chat UI\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Chat UI documentation\n",
      "🤗 Chat UI\n",
      "Chat UI\n",
      "🏡 View all docs\n",
      "AWS Trainium & Inferentia\n",
      "Accelerate\n",
      "Amazon SageMaker\n",
      "Argilla\n",
      "AutoTrain\n",
      "Bitsandbytes\n",
      "Chat UI\n",
      "Dataset viewer\n",
      "Datasets\n",
      "Diffusers\n",
      "Distilabel\n",
      "Evaluate\n",
      "Gradio\n",
      "Hub\n",
      "Hub Python Library\n",
      "Huggingface.js\n",
      "Inference Endpoints (dedicated)\n",
      "Inference Providers\n",
      "Leaderboards\n",
      "Lighteval\n",
      "Optimum\n",
      "PEFT\n",
      "Safetensors\n",
      "Sentence Transformers\n",
      "TRL\n",
      "Tasks\n",
      "Text Embeddings Inference\n",
      "Text Generation Inference\n",
      "Tokenizers\n",
      "Transformers\n",
      "Transformers.js\n",
      "smolagents\n",
      "timm\n",
      "Search documentation\n",
      "main\n",
      "EN\n",
      "🤗 Chat UI\n",
      "Installation\n",
      "Local\n",
      "Spaces\n",
      "Docker\n",
      "Helm\n",
      "Configuration\n",
      "Overview\n",
      "Theming\n",
      "OpenID\n",
      "Web Search\n",
      "Metrics\n",
      "Text Embedding Models\n",
      "Models\n",
      "Overview\n",
      "Multimodal\n",
      "Tools\n",
      "Providers\n",
      "Anthropic\n",
      "AWS\n",
      "Cloudflare\n",
      "Cohere\n",
      "Google\n",
      "Langserve\n",
      "Llama.cpp\n",
      "Ollama\n",
      "OpenAI\n",
      "TGI\n",
      "Common Issues\n",
      "Developing\n",
      "Architecture\n",
      "Copy HuggingChat\n",
      "Join the Hugging Face community\n",
      "and get access to the augmented documentation experience\n",
      "Collaborate on models, datasets and Spaces\n",
      "Faster examples with accelerated inference\n",
      "Switch between documentation themes\n",
      "Sign Up\n",
      "to get started\n",
      "🤗 Chat UI\n",
      "Open source chat interface with support for tools, web search, multimodal and many API providers. The app uses MongoDB and SvelteKit behind the scenes. Try the live version of the app called\n",
      "HuggingChat on hf.co/chat\n",
      "or\n",
      "setup your own instance\n",
      ".\n",
      "🔧\n",
      "Tools\n",
      ": Function calling with custom tools and support for\n",
      "Zero GPU spaces\n",
      "🔍\n",
      "Web Search\n",
      ": Automated web search, scraping and RAG for all models\n",
      "🐙\n",
      "Multimodal\n",
      ": Accepts image file uploads on supported providers\n",
      "👤\n",
      "OpenID\n",
      ": Optionally setup OpenID for user authentication\n",
      "Tools\n",
      "Web Search\n",
      "Quickstart\n",
      "You can quickly have a locally running chat-ui & LLM text-generation server thanks to chat-ui’s\n",
      "llama.cpp server support\n",
      ".\n",
      "Step 1 (Start llama.cpp server):\n",
      "Copied\n",
      "# install llama.cpp\n",
      "brew install llama.cpp\n",
      "# start llama.cpp server (using hf.co/microsoft/Phi-3-mini-4k-instruct-gguf as an example)\n",
      "llama-server --hf-repo microsoft/Phi-3-mini-4k-instruct-gguf --hf-file Phi-3-mini-4k-instruct-q4.gguf -c 4096\n",
      "A local LLaMA.cpp HTTP Server will start on\n",
      "http://localhost:8080\n",
      ". Read more\n",
      "here\n",
      ".\n",
      "Step 2 (tell chat-ui to use local llama.cpp server):\n",
      "Add the following to your\n",
      ".env.local\n",
      ":\n",
      "Copied\n",
      "MODELS\n",
      "=`[\n",
      "  {\n",
      "\"name\"\n",
      ":\n",
      "\"Local microsoft/Phi-3-mini-4k-instruct-gguf\"\n",
      ",\n",
      "\"tokenizer\"\n",
      ":\n",
      "\"microsoft/Phi-3-mini-4k-instruct-gguf\"\n",
      ",\n",
      "\"preprompt\"\n",
      ":\n",
      "\"\"\n",
      ",\n",
      "\"chatPromptTemplate\"\n",
      ":\n",
      "\"<s>{{preprompt}}{{#each messages}}{{#ifUser}}<|user|>\\n{{content}}<|end|>\\n<|assistant|>\\n{{/ifUser}}{{#ifAssistant}}{{content}}<|end|>\\n{{/ifAssistant}}{{/each}}\"\n",
      ",\n",
      "\"parameters\"\n",
      ": {\n",
      "\"stop\"\n",
      ": [\n",
      "\"<|end|>\"\n",
      ",\n",
      "\"<|endoftext|>\"\n",
      ",\n",
      "\"<|assistant|>\"\n",
      "],\n",
      "\"temperature\"\n",
      ":\n",
      "0.7\n",
      ",\n",
      "\"max_new_tokens\"\n",
      ":\n",
      "1024\n",
      ",\n",
      "\"truncate\"\n",
      ":\n",
      "3071\n",
      "},\n",
      "\"endpoints\"\n",
      ": [{\n",
      "\"type\"\n",
      ":\n",
      "\"llamacpp\"\n",
      ",\n",
      "\"baseURL\"\n",
      ":\n",
      "\"http://localhost:8080\"\n",
      "}],\n",
      "  },\n",
      "]`\n",
      "Read more\n",
      "here\n",
      ".\n",
      "Step 3 (make sure you have MongoDb running locally):\n",
      "Copied\n",
      "docker run -d -p 27017:27017 --name mongo-chatui mongo:latest\n",
      "Read more\n",
      "here\n",
      ".\n",
      "Step 4 (start chat-ui):\n",
      "Copied\n",
      "git\n",
      "clone\n",
      "https://github.com/huggingface/chat-ui\n",
      "cd\n",
      "chat-ui\n",
      "npm install\n",
      "npm run dev -- --open\n",
      "Read more\n",
      "here\n",
      ".\n",
      "<\n",
      ">\n",
      "Update\n",
      "on GitHub\n",
      "Local\n",
      "→\n",
      "🤗\n",
      "Chat UI\n",
      "Quickstart\n",
      "\n",
      "\n",
      "\n",
      "Learn\n",
      "Webpage Title:\n",
      "Hugging Face - Learn\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Learn\n",
      "LLM Course\n",
      "This course will teach you about large language models using libraries from the HF ecosystem\n",
      "MCP Course\n",
      "This course will teach you about Model Context Protocol\n",
      "Agents Course\n",
      "Learn to build and deploy your own AI agents\n",
      "Deep RL Course\n",
      "This course will teach you about deep reinforcement learning using libraries from the HF ecosystem\n",
      "Community Computer Vision Course\n",
      "This course will teach you about computer vision ML using libraries and models from the HF ecosystem\n",
      "Audio Course\n",
      "Learn to apply transformers to audio data using libraries from the HF ecosystem\n",
      "Open-Source AI Cookbook\n",
      "A collection of open-source-powered notebooks by AI builders, for AI builders\n",
      "ML for Games Course\n",
      "This course will teach you about integrating AI models your game and using AI tools in your game development workflow\n",
      "Diffusion Course\n",
      "Learn about diffusion models & how to use them with diffusers\n",
      "ML for 3D Course\n",
      "Learn about 3D ML with libraries from the HF ecosystem\n",
      "System theme\n",
      "Company\n",
      "TOS\n",
      "Privacy\n",
      "About\n",
      "Jobs\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Pricing\n",
      "Docs\n",
      "\n",
      "\n",
      "\n",
      "Blog\n",
      "Webpage Title:\n",
      "Hugging Face – Blog\n",
      "Webpage Contents:\n",
      "Hugging Face\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Community\n",
      "Docs\n",
      "Enterprise\n",
      "Pricing\n",
      "Log In\n",
      "Sign Up\n",
      "Blog, Articles, and discussions\n",
      "New Article\n",
      "Everything\n",
      "community\n",
      "guide\n",
      "open source collab\n",
      "partnerships\n",
      "research\n",
      "NLP\n",
      "Audio\n",
      "CV\n",
      "RL\n",
      "ethics\n",
      "Diffusion\n",
      "Game Development\n",
      "RLHF\n",
      "Leaderboard\n",
      "Case Studies\n",
      "LeRobot\n",
      "Microsoft and Hugging Face expand collaboration\n",
      "By\n",
      "jeffboudier\n",
      "May 19, 2025\n",
      "•\n",
      "14\n",
      "Community Articles\n",
      "view all\n",
      "Falcon-Edge: A series of powerful, universal, fine-tunable 1.58bit language models.\n",
      "By\n",
      "tiiuae\n",
      "and 9 others\n",
      "•\n",
      "6 days ago\n",
      "•\n",
      "32\n",
      "TinyAgents: A Minimal Experiment with Code Agents and MCP Tools\n",
      "By\n",
      "albertvillanova\n",
      "•\n",
      "5 days ago\n",
      "•\n",
      "26\n",
      "Uncensor any LLM with abliteration\n",
      "By\n",
      "mlabonne\n",
      "•\n",
      "Jun 13, 2024\n",
      "•\n",
      "573\n",
      "NVIDIA Cosmos Now Available On Hugging Face For Physical AI Reasoning\n",
      "By\n",
      "PranjaliJoshi\n",
      "and 1 other\n",
      "•\n",
      "2 days ago\n",
      "•\n",
      "21\n",
      "🦸🏻#14: What Is MCP, and Why Is Everyone – Suddenly!– Talking About It?\n",
      "By\n",
      "Kseniase\n",
      "•\n",
      "Mar 17\n",
      "•\n",
      "259\n",
      "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance\n",
      "By\n",
      "tiiuae\n",
      "and 6 others\n",
      "•\n",
      "about 5 hours ago\n",
      "•\n",
      "15\n",
      "NH Prediction: Advanced AI System for Korean Agricultural Price Forecasting Research\n",
      "By\n",
      "openfree\n",
      "•\n",
      "about 3 hours ago\n",
      "•\n",
      "13\n",
      "CycleNavigator: Visualizing Economic and Political Cycles Through AI\n",
      "By\n",
      "openfree\n",
      "•\n",
      "8 days ago\n",
      "•\n",
      "21\n",
      "Highlights from the First ICLR 2025 Watermarking Workshop\n",
      "By\n",
      "hadyelsahar\n",
      "and 4 others\n",
      "•\n",
      "7 days ago\n",
      "•\n",
      "9\n",
      "Good answers are not necessarily factual answers: an analysis of hallucination in leading LLMs\n",
      "By\n",
      "davidberenstein1957\n",
      "and 1 other\n",
      "•\n",
      "14 days ago\n",
      "•\n",
      "28\n",
      "All LLMs Will Be Sparse BitNet Hybrids\n",
      "By\n",
      "codys12\n",
      "•\n",
      "7 days ago\n",
      "•\n",
      "9\n",
      "Code a simple RAG from scratch\n",
      "By\n",
      "ngxson\n",
      "•\n",
      "Oct 29, 2024\n",
      "•\n",
      "77\n",
      "The Large Language Model Course\n",
      "By\n",
      "mlabonne\n",
      "•\n",
      "Jan 16\n",
      "•\n",
      "179\n",
      "I trained a Language Model to schedule events with GRPO!\n",
      "By\n",
      "anakin87\n",
      "•\n",
      "22 days ago\n",
      "•\n",
      "72\n",
      "🥬 LettuceDetect Goes Multilingual: Fine-tuning EuroBERT on Synthetic Translations\n",
      "By\n",
      "adaamko\n",
      "and 1 other\n",
      "•\n",
      "2 days ago\n",
      "•\n",
      "6\n",
      "ColPali: Efficient Document Retrieval with Vision Language Models 👀\n",
      "By\n",
      "manu\n",
      "•\n",
      "Jul 5, 2024\n",
      "•\n",
      "252\n",
      "Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth\n",
      "By\n",
      "mlabonne\n",
      "•\n",
      "Jul 29, 2024\n",
      "•\n",
      "320\n",
      "DeepSeek-R1 Dissection: Understanding PPO & GRPO Without Any Prior Reinforcement Learning Knowledge\n",
      "By\n",
      "NormalUhr\n",
      "•\n",
      "Feb 7\n",
      "•\n",
      "137\n",
      "OpenEvolve: An Open Source Implementation of Google DeepMind's AlphaEvolve\n",
      "By\n",
      "codelion\n",
      "•\n",
      "about 19 hours ago\n",
      "•\n",
      "5\n",
      "Falcon-Arabic: A Breakthrough in Arabic Language Models\n",
      "By\n",
      "tiiuae\n",
      "and 6 others\n",
      "•\n",
      "about 5 hours ago\n",
      "•\n",
      "5\n",
      "The Transformers Library: standardizing model definitions\n",
      "By\n",
      "lysandre\n",
      "May 15, 2025\n",
      "•\n",
      "94\n",
      "Improving Hugging Face Model Access for Kaggle Users\n",
      "By\n",
      "roseberryv\n",
      "May 14, 2025\n",
      "•\n",
      "23\n",
      "Blazingly fast whisper transcriptions with Inference Endpoints\n",
      "By\n",
      "mfuntowicz\n",
      "May 13, 2025\n",
      "•\n",
      "62\n",
      "Vision Language Models (Better, Faster, Stronger)\n",
      "By\n",
      "merve\n",
      "May 12, 2025\n",
      "•\n",
      "347\n",
      "LeRobot Community Datasets: The “ImageNet” of Robotics — When and How?\n",
      "By\n",
      "danaaubakirova\n",
      "May 11, 2025\n",
      "•\n",
      "46\n",
      "How to Build an MCP Server with Gradio\n",
      "By\n",
      "abidlabs\n",
      "April 30, 2025\n",
      "•\n",
      "108\n",
      "Welcoming Llama Guard 4 on Hugging Face Hub\n",
      "By\n",
      "merve\n",
      "April 29, 2025\n",
      "•\n",
      "36\n",
      "The 4 Things Qwen-3's Chat Template Teaches Us\n",
      "By\n",
      "cfahlgren1\n",
      "April 30, 2025\n",
      "•\n",
      "42\n",
      "Tiny Agents: a MCP-powered agent in 50 lines of code\n",
      "By\n",
      "julien-c\n",
      "April 25, 2025\n",
      "•\n",
      "249\n",
      "Introducing AutoRound: Intel’s Advanced Quantization for LLMs and VLMs\n",
      "By\n",
      "wenhuach\n",
      "April 29, 2025\n",
      "•\n",
      "28\n",
      "17 Reasons Why Gradio Isn't Just Another UI Library\n",
      "By\n",
      "ysharma\n",
      "April 16, 2025\n",
      "•\n",
      "34\n",
      "Cohere on Hugging Face Inference Providers 🔥\n",
      "By\n",
      "burtenshaw\n",
      "April 16, 2025\n",
      "•\n",
      "126\n",
      "Introducing HELMET\n",
      "By\n",
      "hyen\n",
      "April 16, 2025\n",
      "•\n",
      "29\n",
      "Hugging Face to sell open-source robots thanks to Pollen Robotics acquisition 🤖\n",
      "By\n",
      "thomwolf\n",
      "April 14, 2025\n",
      "•\n",
      "46\n",
      "Previous\n",
      "1\n",
      "2\n",
      "3\n",
      "...\n",
      "41\n",
      "Next\n",
      "Community Articles\n",
      "Sort: \n",
      "\t\tTrending\n",
      "Falcon-Edge: A series of powerful, universal, fine-tunable 1.58bit language models.\n",
      "By\n",
      "tiiuae\n",
      "and 9 others\n",
      "•\n",
      "6 days ago\n",
      "•\n",
      "32\n",
      "TinyAgents: A Minimal Experiment with Code Agents and MCP Tools\n",
      "By\n",
      "albertvillanova\n",
      "•\n",
      "5 days ago\n",
      "•\n",
      "26\n",
      "Uncensor any LLM with abliteration\n",
      "By\n",
      "mlabonne\n",
      "•\n",
      "Jun 13, 2024\n",
      "•\n",
      "573\n",
      "NVIDIA Cosmos Now Available On Hugging Face For Physical AI Reasoning\n",
      "By\n",
      "PranjaliJoshi\n",
      "and 1 other\n",
      "•\n",
      "2 days ago\n",
      "•\n",
      "21\n",
      "🦸🏻#14: What Is MCP, and Why Is Everyone – Suddenly!– Talking About It?\n",
      "By\n",
      "Kseniase\n",
      "•\n",
      "Mar 17\n",
      "•\n",
      "259\n",
      "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance\n",
      "By\n",
      "tiiuae\n",
      "and 6 others\n",
      "•\n",
      "about 5 hours ago\n",
      "•\n",
      "15\n",
      "NH Prediction: Advanced AI System for Korean Agricultural Price Forecasting Research\n",
      "By\n",
      "openfree\n",
      "•\n",
      "about 3 hours ago\n",
      "•\n",
      "13\n",
      "CycleNavigator: Visualizing Economic and Political Cycles Through AI\n",
      "By\n",
      "openfree\n",
      "•\n",
      "8 days ago\n",
      "•\n",
      "21\n",
      "Highlights from the First ICLR 2025 Watermarking Workshop\n",
      "By\n",
      "hadyelsahar\n",
      "and 4 others\n",
      "•\n",
      "7 days ago\n",
      "•\n",
      "9\n",
      "Good answers are not necessarily factual answers: an analysis of hallucination in leading LLMs\n",
      "By\n",
      "davidberenstein1957\n",
      "and 1 other\n",
      "•\n",
      "14 days ago\n",
      "•\n",
      "28\n",
      "All LLMs Will Be Sparse BitNet Hybrids\n",
      "By\n",
      "codys12\n",
      "•\n",
      "7 days ago\n",
      "•\n",
      "9\n",
      "Code a simple RAG from scratch\n",
      "By\n",
      "ngxson\n",
      "•\n",
      "Oct 29, 2024\n",
      "•\n",
      "77\n",
      "The Large Language Model Course\n",
      "By\n",
      "mlabonne\n",
      "•\n",
      "Jan 16\n",
      "•\n",
      "179\n",
      "I trained a Language Model to schedule events with GRPO!\n",
      "By\n",
      "anakin87\n",
      "•\n",
      "22 days ago\n",
      "•\n",
      "72\n",
      "🥬 LettuceDetect Goes Multilingual: Fine-tuning EuroBERT on Synthetic Translations\n",
      "By\n",
      "adaamko\n",
      "and 1 other\n",
      "•\n",
      "2 days ago\n",
      "•\n",
      "6\n",
      "ColPali: Efficient Document Retrieval with Vision Language Models 👀\n",
      "By\n",
      "manu\n",
      "•\n",
      "Jul 5, 2024\n",
      "•\n",
      "252\n",
      "Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth\n",
      "By\n",
      "mlabonne\n",
      "•\n",
      "Jul 29, 2024\n",
      "•\n",
      "320\n",
      "DeepSeek-R1 Dissection: Understanding PPO & GRPO Without Any Prior Reinforcement Learning Knowledge\n",
      "By\n",
      "NormalUhr\n",
      "•\n",
      "Feb 7\n",
      "•\n",
      "137\n",
      "OpenEvolve: An Open Source Implementation of Google DeepMind's AlphaEvolve\n",
      "By\n",
      "codelion\n",
      "•\n",
      "about 19 hours ago\n",
      "•\n",
      "5\n",
      "Falcon-Arabic: A Breakthrough in Arabic Language Models\n",
      "By\n",
      "tiiuae\n",
      "and 6 others\n",
      "•\n",
      "about 5 hours ago\n",
      "•\n",
      "5\n",
      "View all\n",
      "System theme\n",
      "Company\n",
      "TOS\n",
      "Privacy\n",
      "About\n",
      "Jobs\n",
      "Website\n",
      "Models\n",
      "Datasets\n",
      "Spaces\n",
      "Pricing\n",
      "Docs\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_all_details(\"https://huggingface.co/docs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "54642b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a technical assistant that analyzes the contents of several relevant pages from a Documentation website \\\n",
    "and creates a quickstart guide for users, Road map to using the tool, and a table of all necessary commands and scripts. Respond in markdown.\\\n",
    "Include details of how to use the tool and make it user-friendly for beginners.\"\n",
    "\n",
    "# Or uncomment the lines below for a more humorous brochure - this demonstrates how easy it is to incorporate 'tone':\n",
    "\n",
    "# system_prompt = \"You are an assistant that analyzes the contents of several relevant pages from a company website \\\n",
    "# and creates a short humorous, entertaining, jokey brochure about the company for prospective customers, investors and recruits. Respond in markdown.\\\n",
    "# Include details of company culture, customers and careers/jobs if you have the information.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7f032f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quickstart_user_prompt(tool_name, url):\n",
    "    user_prompt = f\"You are looking at a tool called: {tool_name}\\n\"\n",
    "    user_prompt += f\"Here are the contents of its landing page and other relevant pages; use this information to build a short quickstart guide of the tool in markdown.\\n\"\n",
    "    user_prompt += get_all_details(url)\n",
    "    user_prompt = user_prompt[:20_000] # Truncate if more than 20,000 characters\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4da62d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found links: {'links': [{'type': 'Models', 'url': 'https://huggingface.co/docs/models'}, {'type': 'Datasets', 'url': 'https://huggingface.co/docs/datasets'}, {'type': 'Spaces', 'url': 'https://huggingface.co/docs/spaces'}, {'type': 'Documentation', 'url': 'https://huggingface.co/docs'}, {'type': 'Hugging Face Hub', 'url': 'https://huggingface.co/docs/huggingface_hub'}, {'type': 'Inference Providers', 'url': 'https://huggingface.co/docs/inference-providers'}, {'type': 'Text Generation Inference', 'url': 'https://huggingface.co/docs/text-generation-inference'}, {'type': 'Text Embeddings Inference', 'url': 'https://huggingface.co/docs/text-embeddings-inference'}, {'type': 'Transformers', 'url': 'https://huggingface.co/docs/transformers'}, {'type': 'Diffusers', 'url': 'https://huggingface.co/docs/diffusers'}, {'type': 'Evaluate', 'url': 'https://huggingface.co/docs/evaluate'}, {'type': 'Training and Fine-tuning', 'url': 'https://huggingface.co/docs/transformers.js'}, {'type': 'Accelerate', 'url': 'https://huggingface.co/docs/accelerate'}, {'type': 'Optimum', 'url': 'https://huggingface.co/docs/optimum'}, {'type': 'Auto Train', 'url': 'https://huggingface.co/docs/autotrain'}, {'type': 'Chat UI', 'url': 'https://huggingface.co/docs/chat-ui'}, {'type': 'Learn', 'url': 'https://huggingface.co/learn'}, {'type': 'Blog', 'url': 'https://huggingface.co/blog'}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You are looking at a tool called: huggingface\\nHere are the contents of its landing page and other relevant pages; use this information to build a short quickstart guide of the tool in markdown.\\nLanding page:\\nWebpage Title:\\nHugging Face - Documentation\\nWebpage Contents:\\nHugging Face\\nModels\\nDatasets\\nSpaces\\nCommunity\\nDocs\\nEnterprise\\nPricing\\nLog In\\nSign Up\\nDocumentations\\nHub & Client Libraries\\nHub\\nHost Git-based models, datasets, and Spaces on the HF Hub\\nHub Python Library\\nPython client to interact with the Hugging Face Hub\\nHuggingface.js\\nJavaScript libraries for Hugging Face with built-in TS types\\nTasks\\nExplore demos, models, and datasets for any ML tasks\\nDataset viewer\\nAPI for metadata, stats, and content of HF Hub datasets\\nDeployment & Inference\\nInference Providers\\nCall 200k+ models hosted by our 10+ Inference partners\\nInference Endpoints (dedicated)\\nDeploy models on dedicated & fully managed infrastructure on HF\\nAmazon SageMaker\\nTrain/deploy Transformers models with SageMaker/HF DLCs\\nText Generation Inference\\nServe language models with TGI optimized toolkit\\nText Embeddings Inference\\nServe embeddings models with TEI optimized toolkit\\nCore ML Libraries\\nTransformers\\nState-of-the-art ML for PyTorch, TensorFlow, JAX\\nDiffusers\\nState-of-the-art Diffusion models in PyTorch\\nDatasets\\nAccess & share datasets for any ML tasks\\nTransformers.js\\nState-of-the-art ML running directly in your browser\\nTokenizers\\nFast tokenizers optimized for research & production\\nEvaluate\\nEvaluate and compare models performance\\ntimm\\nState-of-the-art vision models: layers, optimizers, and utilities\\nSentence Transformers\\nEmbeddings, Retrieval, and Reranking\\nTraining & Optimization\\nPEFT\\nParameter-efficient finetuning for large language models\\nAccelerate\\nTrain PyTorch models with multi-GPU, TPU, mixed precision\\nOptimum\\nOptimize HF Transformers for faster training/inference\\nAWS Trainium & Inferentia\\nTrain/deploy Transformers/Diffusers on AWS\\nTRL\\nTrain transformers LMs with reinforcement learning\\nSafetensors\\nSafe way to store/distribute neural network weights\\nBitsandbytes\\nOptimize and quantize models with bitsandbytes\\nLighteval\\nAll-in-one toolkit to evaluate LLMs across multiple backends\\nCollaboration & Extras\\nGradio\\nBuild ML demos and web apps with a few lines of Python\\nsmolagents\\nSmol library to build great agents in Python\\nAutoTrain\\nAutoTrain API and UI for seamless model training\\nChat UI\\nOpen source chat frontend powering HuggingChat\\nLeaderboards\\nCreate custom Leaderboards on Hugging Face\\nArgilla\\nCollaboration tool for building high-quality datasets\\nDistilabel\\nFramework for synthetic data generation and AI feedback\\nCommunity\\nBlog\\nLearn\\nDiscord\\nForum\\nGithub\\nSystem theme\\nCompany\\nTOS\\nPrivacy\\nAbout\\nJobs\\nWebsite\\nModels\\nDatasets\\nSpaces\\nPricing\\nDocs\\n\\n\\n\\nModels\\nWebpage Title:\\n404 – Hugging Face\\nWebpage Contents:\\nHugging Face\\nModels\\nDatasets\\nSpaces\\nCommunity\\nDocs\\nEnterprise\\nPricing\\nLog In\\nSign Up\\n404\\nSorry, we can\\'t find the page you are looking for.\\nSystem theme\\nWebsite\\nModels\\nDatasets\\nSpaces\\nTasks\\nInference Endpoints\\nHuggingChat\\nCompany\\nAbout\\nBrand assets\\nTerms of service\\nPrivacy\\nJobs\\nPress\\nResources\\nLearn\\nDocumentation\\nBlog\\nForum\\nService Status\\nSocial\\nGitHub\\nTwitter\\nLinkedIn\\nDiscord\\n\\n\\n\\nDatasets\\nWebpage Title:\\nDatasets\\nWebpage Contents:\\nHugging Face\\nModels\\nDatasets\\nSpaces\\nCommunity\\nDocs\\nEnterprise\\nPricing\\nLog In\\nSign Up\\nDatasets documentation\\nDatasets\\nDatasets\\n🏡 View all docs\\nAWS Trainium & Inferentia\\nAccelerate\\nAmazon SageMaker\\nArgilla\\nAutoTrain\\nBitsandbytes\\nChat UI\\nDataset viewer\\nDatasets\\nDiffusers\\nDistilabel\\nEvaluate\\nGradio\\nHub\\nHub Python Library\\nHuggingface.js\\nInference Endpoints (dedicated)\\nInference Providers\\nLeaderboards\\nLighteval\\nOptimum\\nPEFT\\nSafetensors\\nSentence Transformers\\nTRL\\nTasks\\nText Embeddings Inference\\nText Generation Inference\\nTokenizers\\nTransformers\\nTransformers.js\\nsmolagents\\ntimm\\nSearch documentation\\nmain\\nv3.6.0\\nv3.5.1\\nv3.4.1\\nv3.3.2\\nv3.2.0\\nv3.1.0\\nv3.0.2\\nv2.21.0\\nv2.20.0\\nv2.19.0\\nv2.18.0\\nv2.17.1\\nv2.16.1\\nv2.15.0\\nv2.14.7\\nv2.13.2\\nv2.12.0\\nv2.11.0\\nv2.10.0\\nv2.9.0\\nv2.8.0\\nv2.7.1\\nv2.6.2\\nv2.5.2\\nv2.4.0\\nv2.3.2\\nv2.2.1\\nv2.1.0\\nv2.0.0\\nv1.18.3\\nv1.17.0\\nv1.16.1\\nv1.15.1\\nv1.14.0\\nv1.13.3\\nv1.12.1\\nv1.11.0\\nv1.10.2\\nv1.9.0\\nv1.8.0\\nv1.7.0\\nv1.6.2\\nv1.5.0\\nv1.4.1\\nv1.3.0\\nv1.2.1\\nv1.1.3\\nv1.0.2\\nv0.4.0\\nv0.3.0\\nEN\\nGet started\\n🤗 Datasets\\nQuickstart\\nInstallation\\nTutorials\\nOverview\\nLoad a dataset from the Hub\\nKnow your dataset\\nPreprocess\\nCreate a dataset\\nShare a dataset to the Hub\\nHow-to guides\\nOverview\\nGeneral usage\\nLoad\\nProcess\\nStream\\nUse with PyTorch\\nUse with TensorFlow\\nUse with NumPy\\nUse with JAX\\nUse with Pandas\\nUse with Polars\\nUse with PyArrow\\nUse with Spark\\nCache management\\nCloud storage\\nSearch index\\nCLI\\nTroubleshooting\\nAudio\\nLoad audio data\\nProcess audio data\\nCreate an audio dataset\\nVision\\nLoad image data\\nProcess image data\\nCreate an image dataset\\nDepth estimation\\nImage classification\\nSemantic segmentation\\nObject detection\\nLoad video data\\nCreate a video dataset\\nLoad document data\\nCreate a document dataset\\nText\\nLoad text data\\nProcess text data\\nTabular\\nLoad tabular data\\nDataset repository\\nShare\\nCreate a dataset card\\nStructure your repository\\nCreate a dataset loading script\\nConceptual guides\\nDatasets 🤝 Arrow\\nThe cache\\nDataset or IterableDataset\\nDataset features\\nBuild and load\\nBatch mapping\\nReference\\nMain classes\\nBuilder classes\\nLoading methods\\nTable Classes\\nUtilities\\nJoin the Hugging Face community\\nand get access to the augmented documentation experience\\nCollaborate on models, datasets and Spaces\\nFaster examples with accelerated inference\\nSwitch between documentation themes\\nSign Up\\nto get started\\nDatasets\\n🤗 Datasets is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks.\\nLoad a dataset in a single line of code, and use our powerful data processing methods to quickly get your dataset ready for training in a deep learning model. Backed by the Apache Arrow format, process large datasets with zero-copy reads without any memory constraints for optimal speed and efficiency. We also feature a deep integration with the\\nHugging Face Hub\\n, allowing you to easily load and share a dataset with the wider machine learning community.\\nFind your dataset today on the\\nHugging Face Hub\\n, and take an in-depth look inside of it with the live viewer.\\nTutorials\\nLearn the basics and become familiar with loading, accessing, and processing a dataset. Start here if you are using 🤗 Datasets for the first time!\\nHow-to guides\\nPractical guides to help you achieve a specific goal. Take a look at these guides to learn how to use 🤗 Datasets to solve real-world problems.\\nConceptual guides\\nHigh-level explanations for building a better understanding about important topics such as the underlying data format, the cache, and how datasets are generated.\\nReference\\nTechnical descriptions of how 🤗 Datasets classes and methods work.\\n<\\n>\\nUpdate\\non GitHub\\nQuickstart\\n→\\nDatasets\\n\\n\\n\\nSpaces\\nWebpage Title:\\n404 – Hugging Face\\nWebpage Contents:\\nHugging Face\\nModels\\nDatasets\\nSpaces\\nCommunity\\nDocs\\nEnterprise\\nPricing\\nLog In\\nSign Up\\n404\\nSorry, we can\\'t find the page you are looking for.\\nSystem theme\\nWebsite\\nModels\\nDatasets\\nSpaces\\nTasks\\nInference Endpoints\\nHuggingChat\\nCompany\\nAbout\\nBrand assets\\nTerms of service\\nPrivacy\\nJobs\\nPress\\nResources\\nLearn\\nDocumentation\\nBlog\\nForum\\nService Status\\nSocial\\nGitHub\\nTwitter\\nLinkedIn\\nDiscord\\n\\n\\n\\nDocumentation\\nWebpage Title:\\nHugging Face - Documentation\\nWebpage Contents:\\nHugging Face\\nModels\\nDatasets\\nSpaces\\nCommunity\\nDocs\\nEnterprise\\nPricing\\nLog In\\nSign Up\\nDocumentations\\nHub & Client Libraries\\nHub\\nHost Git-based models, datasets, and Spaces on the HF Hub\\nHub Python Library\\nPython client to interact with the Hugging Face Hub\\nHuggingface.js\\nJavaScript libraries for Hugging Face with built-in TS types\\nTasks\\nExplore demos, models, and datasets for any ML tasks\\nDataset viewer\\nAPI for metadata, stats, and content of HF Hub datasets\\nDeployment & Inference\\nInference Providers\\nCall 200k+ models hosted by our 10+ Inference partners\\nInference Endpoints (dedicated)\\nDeploy models on dedicated & fully managed infrastructure on HF\\nAmazon SageMaker\\nTrain/deploy Transformers models with SageMaker/HF DLCs\\nText Generation Inference\\nServe language models with TGI optimized toolkit\\nText Embeddings Inference\\nServe embeddings models with TEI optimized toolkit\\nCore ML Libraries\\nTransformers\\nState-of-the-art ML for PyTorch, TensorFlow, JAX\\nDiffusers\\nState-of-the-art Diffusion models in PyTorch\\nDatasets\\nAccess & share datasets for any ML tasks\\nTransformers.js\\nState-of-the-art ML running directly in your browser\\nTokenizers\\nFast tokenizers optimized for research & production\\nEvaluate\\nEvaluate and compare models performance\\ntimm\\nState-of-the-art vision models: layers, optimizers, and utilities\\nSentence Transformers\\nEmbeddings, Retrieval, and Reranking\\nTraining & Optimization\\nPEFT\\nParameter-efficient finetuning for large language models\\nAccelerate\\nTrain PyTorch models with multi-GPU, TPU, mixed precision\\nOptimum\\nOptimize HF Transformers for faster training/inference\\nAWS Trainium & Inferentia\\nTrain/deploy Transformers/Diffusers on AWS\\nTRL\\nTrain transformers LMs with reinforcement learning\\nSafetensors\\nSafe way to store/distribute neural network weights\\nBitsandbytes\\nOptimize and quantize models with bitsandbytes\\nLighteval\\nAll-in-one toolkit to evaluate LLMs across multiple backends\\nCollaboration & Extras\\nGradio\\nBuild ML demos and web apps with a few lines of Python\\nsmolagents\\nSmol library to build great agents in Python\\nAutoTrain\\nAutoTrain API and UI for seamless model training\\nChat UI\\nOpen source chat frontend powering HuggingChat\\nLeaderboards\\nCreate custom Leaderboards on Hugging Face\\nArgilla\\nCollaboration tool for building high-quality datasets\\nDistilabel\\nFramework for synthetic data generation and AI feedback\\nCommunity\\nBlog\\nLearn\\nDiscord\\nForum\\nGithub\\nSystem theme\\nCompany\\nTOS\\nPrivacy\\nAbout\\nJobs\\nWebsite\\nModels\\nDatasets\\nSpaces\\nPricing\\nDocs\\n\\n\\n\\nHugging Face Hub\\nWebpage Title:\\n🤗 Hub client library\\nWebpage Contents:\\nHugging Face\\nModels\\nDatasets\\nSpaces\\nCommunity\\nDocs\\nEnterprise\\nPricing\\nLog In\\nSign Up\\nHub Python Library documentation\\n🤗 Hub client library\\nHub Python Library\\n🏡 View all docs\\nAWS Trainium & Inferentia\\nAccelerate\\nAmazon SageMaker\\nArgilla\\nAutoTrain\\nBitsandbytes\\nChat UI\\nDataset viewer\\nDatasets\\nDiffusers\\nDistilabel\\nEvaluate\\nGradio\\nHub\\nHub Python Library\\nHuggingface.js\\nInference Endpoints (dedicated)\\nInference Providers\\nLeaderboards\\nLighteval\\nOptimum\\nPEFT\\nSafetensors\\nSentence Transformers\\nTRL\\nTasks\\nText Embeddings Inference\\nText Generation Inference\\nTokenizers\\nTransformers\\nTransformers.js\\nsmolagents\\ntimm\\nSearch documentation\\nmain\\nv0.31.4\\nv0.30.2\\nv0.29.3\\nv0.28.1\\nv0.27.1\\nv0.26.5\\nv0.25.2\\nv0.24.7\\nv0.23.5\\nv0.22.2\\nv0.21.4\\nv0.20.3\\nv0.19.3\\nv0.18.0.rc0\\nv0.17.3\\nv0.16.3\\nv0.15.1\\nv0.14.1\\nv0.13.4\\nv0.12.1\\nv0.11.0\\nv0.10.1\\nv0.9.1\\nv0.8.1\\nv0.7.0.rc0\\nv0.6.0.rc0\\nv0.5.1\\nCN\\nDE\\nEN\\nFR\\nHI\\nKO\\nTM\\nGet started\\nHome\\nQuickstart\\nInstallation\\nHow-to guides\\nOverview\\nDownload files\\nUpload files\\nUse the CLI\\nHfFileSystem\\nRepository\\nSearch\\nInference\\nInference Endpoints\\nCommunity Tab\\nCollections\\nCache\\nModel Cards\\nManage your Space\\nIntegrate a library\\nWebhooks\\nConceptual guides\\nGit vs HTTP paradigm\\nReference\\nOverview\\nAuthentication\\nEnvironment variables\\nManaging local and online repositories\\nHugging Face Hub API\\nDownloading files\\nMixins & serialization methods\\nInference Types\\nInference Client\\nInference Endpoints\\nHfFileSystem\\nUtilities\\nDiscussions and Pull Requests\\nCache-system reference\\nRepo Cards and Repo Card Data\\nSpace runtime\\nCollections\\nTensorBoard logger\\nWebhooks server\\nSerialization\\nStrict dataclasses\\nJoin the Hugging Face community\\nand get access to the augmented documentation experience\\nCollaborate on models, datasets and Spaces\\nFaster examples with accelerated inference\\nSwitch between documentation themes\\nSign Up\\nto get started\\n🤗 Hub client library\\nThe\\nhuggingface_hub\\nlibrary allows you to interact with the\\nHugging Face\\nHub\\n, a machine learning platform for creators and collaborators.\\nDiscover pre-trained models and datasets for your projects or play with the hundreds of\\nmachine learning apps hosted on the Hub. You can also create and share your own models\\nand datasets with the community. The\\nhuggingface_hub\\nlibrary provides a simple way to\\ndo all these things with Python.\\nRead the\\nquick start guide\\nto get up and running with the\\nhuggingface_hub\\nlibrary. You will learn how to download files from the Hub, create a\\nrepository, and upload files to the Hub. Keep reading to learn more about how to manage\\nyour repositories on the 🤗 Hub, how to interact in discussions or even how to access\\nthe Inference API.\\nHow-to guides\\nPractical guides to help you achieve a specific goal. Take a look at these guides to learn how to use huggingface_hub to solve real-world problems.\\nReference\\nExhaustive and technical description of huggingface_hub classes and methods.\\nConceptual guides\\nHigh-level explanations for building a better understanding of huggingface_hub philosophy.\\nContribute\\nAll contributions to the\\nhuggingface_hub\\nare welcomed and equally valued! 🤗 Besides\\nadding or fixing existing issues in the code, you can also help improve the\\ndocumentation by making sure it is accurate and up-to-date, help answer questions on\\nissues, and request new features you think will improve the library. Take a look at the\\ncontribution\\nguide\\nto\\nlearn more about how to submit a new issue or feature request, how to submit a pull\\nrequest, and how to test your contributions to make sure everything works as expected.\\nContributors should also be respectful of our\\ncode of\\nconduct\\nto\\ncreate an inclusive and welcoming collaborative space for everyone.\\n<\\n>\\nUpdate\\non GitHub\\nQuickstart\\n→\\n🤗\\nHub client library\\nContribute\\n\\n\\n\\nInference Providers\\nWebpage Title:\\nInference Providers\\nWebpage Contents:\\nHugging Face\\nModels\\nDatasets\\nSpaces\\nCommunity\\nDocs\\nEnterprise\\nPricing\\nLog In\\nSign Up\\nInference Providers documentation\\nInference Providers\\nInference Providers\\n🏡 View all docs\\nAWS Trainium & Inferentia\\nAccelerate\\nAmazon SageMaker\\nArgilla\\nAutoTrain\\nBitsandbytes\\nChat UI\\nDataset viewer\\nDatasets\\nDiffusers\\nDistilabel\\nEvaluate\\nGradio\\nHub\\nHub Python Library\\nHuggingface.js\\nInference Endpoints (dedicated)\\nInference Providers\\nLeaderboards\\nLighteval\\nOptimum\\nPEFT\\nSafetensors\\nSentence Transformers\\nTRL\\nTasks\\nText Embeddings Inference\\nText Generation Inference\\nTokenizers\\nTransformers\\nTransformers.js\\nsmolagents\\ntimm\\nSearch documentation\\nmain\\nEN\\nGet Started\\nInference Providers\\nPricing and Billing\\nHub integration\\nRegister as an Inference Provider\\nSecurity\\nProviders\\nCerebras\\nCohere\\nFal AI\\nFireworks\\nHyperbolic\\nHF Inference\\nNebius\\nNovita\\nNscale\\nReplicate\\nSambaNova\\nTogether\\nAPI Reference\\nIndex\\nHub API\\nPopular Tasks\\nChat Completion\\nFeature Extraction\\nText to Image\\nText to Video\\nOther Tasks\\nJoin the Hugging Face community\\nand get access to the augmented documentation experience\\nCollaborate on models, datasets and Spaces\\nFaster examples with accelerated inference\\nSwitch between documentation themes\\nSign Up\\nto get started\\nInference Providers\\nHugging Face’s Inference Providers give developers streamlined, unified access to hundreds of machine learning models, powered by our serverless inference partners. This new approach builds on our previous Serverless Inference API, offering more models, improved performance, and greater reliability thanks to world-class providers.\\nTo learn more about the launch of Inference Providers, check out our\\nannouncement blog post\\n.\\nPartners\\nHere is the complete list of partners integrated with Inference Providers, and the supported tasks for each of them:\\nProvider\\nChat completion (LLM)\\nChat completion (VLM)\\nFeature Extraction\\nText to Image\\nText to video\\nCerebras\\n✅\\nCohere\\n✅\\n✅\\nFal AI\\n✅\\n✅\\nFireworks\\n✅\\n✅\\nHF Inference\\n✅\\n✅\\n✅\\n✅\\nHyperbolic\\n✅\\n✅\\nNebius\\n✅\\n✅\\n✅\\n✅\\nNovita\\n✅\\n✅\\n✅\\nNscale\\n✅\\n✅\\n✅\\nReplicate\\n✅\\n✅\\nSambaNova\\n✅\\n✅\\nTogether\\n✅\\n✅\\n✅\\nWhy use Inference Providers?\\nInference Providers offers a fast and simple way to explore thousands of models for a variety of tasks. Whether you’re experimenting with ML capabilities or building a new application, this API gives you instant access to high-performing models across multiple domains:\\nText Generation:\\nIncluding large language models and tool-calling prompts, generate and experiment with high-quality responses.\\nImage and Video Generation:\\nEasily create customized images, including LoRAs for your own styles.\\nDocument Embeddings:\\nBuild search and retrieval systems with SOTA embeddings.\\nClassical AI Tasks:\\nReady-to-use models for text classification, image classification, speech recognition, and more.\\n⚡\\nFast and Free to Get Started\\n: Inference Providers comes with a free-tier and additional included credits for\\nPRO users\\n, as well as\\nEnterprise Hub organizations\\n.\\nKey Features\\n🎯 All-in-One API\\n: A single API for text generation, image generation, document embeddings, NER, summarization, image classification, and more.\\n🔀 Multi-Provider Support\\n: Easily run models from top-tier providers like fal, Replicate, Sambanova, Together AI, and others.\\n🚀 Scalable & Reliable\\n: Built for high availability and low-latency performance in production environments.\\n🔧 Developer-Friendly\\n: Simple requests, fast responses, and a consistent developer experience across Python and JavaScript clients.\\n💰 Cost-Effective\\n: No extra markup on provider rates.\\nInference Playground\\nTo get started quickly with\\nChat Completion models\\n, use the\\nInference Playground\\nto easily test and compare models with your prompts.\\nGet Started\\nYou can use Inference Providers with your preferred tools, such as Python, JavaScript, or cURL. To simplify integration, we offer both a Python SDK (\\nhuggingface_hub\\n) and a JavaScript SDK (\\nhuggingface.js\\n).\\nIn this section, we will demonstrate a simple example using\\ndeepseek-ai/DeepSeek-V3-0324\\n, a conversational Large Language Model. For the example, we will use\\nNovita AI\\nas Inference Provider.\\nYou can also automatically select a provider for a model using\\nprovider=\"auto\"\\n— it will pick the first available provider for your model based on your preferred order set in\\nhttps://hf.co/settings/inference-providers\\n.\\nThis is the default if you don’t specify a provider in our Python or JavaScript SDK.\\nAuthentication\\nInference Providers requires passing a user token in the request headers. You can generate a token by signing up on the Hugging Face website and going to the\\nsettings page\\n. We recommend creating a\\nfine-grained\\ntoken with the scope to\\nMake calls to Inference Providers\\n.\\nFor more details about user tokens, check out\\nthis guide\\n.\\ncURL\\nLet’s start with a cURL command highlighting the raw HTTP request. You can adapt this request to be run with the tool of your choice.\\nCopied\\ncurl https://router.huggingface.co/novita/v3/openai/chat/completions \\\\\\n    -H\\n\"Authorization: Bearer\\n$HF_TOKEN\\n\"\\n\\\\\\n    -H\\n\\'Content-Type: application/json\\'\\n\\\\\\n    -d\\n\\'{\\n        \"messages\": [\\n            {\\n                \"role\": \"user\",\\n                \"content\": \"How many G in huggingface?\"\\n            }\\n        ],\\n        \"model\": \"deepseek/deepseek-v3-0324\",\\n        \"stream\": false\\n    }\\'\\nPython\\nIn Python, you can use the\\nrequests\\nlibrary to make raw requests to the API:\\nCopied\\nimport\\nrequests\\n\\nAPI_URL =\\n\"https://router.huggingface.co/novita/v3/openai/chat/completions\"\\nheaders = {\\n\"Authorization\"\\n:\\n\"Bearer hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\\n}\\npayload = {\\n\"messages\"\\n: [\\n        {\\n\"role\"\\n:\\n\"user\"\\n,\\n\"content\"\\n:\\n\"How many \\'G\\'s in \\'huggingface\\'?\"\\n}\\n    ],\\n\"model\"\\n:\\n\"deepseek/deepseek-v3-0324\"\\n,\\n}\\n\\nresponse = requests.post(API_URL, headers=headers, json=payload)\\nprint\\n(response.json()[\\n\"choices\"\\n][\\n0\\n][\\n\"message\"\\n])\\nFor convenience, the Python library\\nhuggingface_hub\\nprovides an\\nInferenceClient\\nthat handles inference for you. Make sure to install it with\\npip install huggingface_hub\\n.\\nCopied\\nfrom\\nhuggingface_hub\\nimport\\nInferenceClient\\n\\nclient = InferenceClient(\\n    provider=\\n\"novita\"\\n,\\n    api_key=\\n\"hf_xxxxxxxxxxxxxxxxxxxxxxxx\"\\n,\\n)\\n\\ncompletion = client.chat.completions.create(\\n    model=\\n\"deepseek-ai/DeepSeek-V3-0324\"\\n,\\n    messages=[\\n        {\\n\"role\"\\n:\\n\"user\"\\n,\\n\"content\"\\n:'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_quickstart_user_prompt(\"huggingface\", \"https://huggingface.co/docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "14d8e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quickstart_guide(tool_name, url):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_quickstart_user_prompt(tool_name, url)}\n",
    "          ],\n",
    "    )\n",
    "    result = response.choices[0].message.content\n",
    "    display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c3ebfb00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found links: {'links': [{'type': 'Home', 'url': 'https://huggingface.co/docs'}, {'type': 'Models', 'url': 'https://huggingface.co/models'}, {'type': 'Datasets', 'url': 'https://huggingface.co/docs/datasets'}, {'type': 'Spaces', 'url': 'https://huggingface.co/spaces'}, {'type': 'Hub Documentation', 'url': 'https://huggingface.co/docs/hub'}, {'type': 'Hugging Face Hub', 'url': 'https://huggingface.co/docs/huggingface_hub'}, {'type': 'Tasks', 'url': 'https://huggingface.co/docs/tasks'}, {'type': 'Text Generation Inference', 'url': 'https://huggingface.co/docs/text-generation-inference'}, {'type': 'Transformers Documentation', 'url': 'https://huggingface.co/docs/transformers'}, {'type': 'Tokenizers Documentation', 'url': 'https://huggingface.co/docs/tokenizers'}, {'type': 'Evaluation Documentation', 'url': 'https://huggingface.co/docs/evaluate'}, {'type': 'Blog', 'url': 'https://huggingface.co/blog'}, {'type': 'Learn', 'url': 'https://huggingface.co/learn'}]}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Quickstart Guide for Hugging Face\n",
       "\n",
       "Hugging Face is a powerful hub for Machine Learning models, datasets, and applications. This guide will help you get started with Hugging Face and utilize its various features effectively, from model deployment to dataset management.\n",
       "\n",
       "## Getting Started\n",
       "\n",
       "### 1. Sign Up / Log In\n",
       "To access the features of Hugging Face, you need to create an account or log in.\n",
       "- **Sign Up**: Visit the [Hugging Face Sign Up Page](https://huggingface.co/join) and fill out your details.\n",
       "- **Log In**: If you already have an account, click on the **Log In** button on the homepage.\n",
       "\n",
       "### 2. Explore the Hub\n",
       "Browse through the vast collection of models, datasets, and demo applications (Spaces) available on the [Hugging Face Hub](https://huggingface.co/models).\n",
       "\n",
       "### 3. Install the Necessary Libraries\n",
       "Hugging Face offers various libraries for different programming environments. Install relevant packages according to your use case. For Python, install the `transformers` and `datasets` libraries:\n",
       "\n",
       "```bash\n",
       "pip install transformers datasets\n",
       "```\n",
       "\n",
       "### 4. Load a Model\n",
       "To use a model from the Hugging Face Hub, load it with just a few lines of code. Here's how to load a pre-trained model:\n",
       "\n",
       "```python\n",
       "from transformers import pipeline\n",
       "\n",
       "model = pipeline(\"text-generation\", model=\"gpt2\")\n",
       "result = model(\"Hello, I am a model,\", max_length=50)\n",
       "print(result)\n",
       "```\n",
       "\n",
       "### 5. Work with Datasets\n",
       "You can load datasets directly from the Hub. Here's a simple example of loading a dataset:\n",
       "\n",
       "```python\n",
       "from datasets import load_dataset\n",
       "\n",
       "dataset = load_dataset(\"imdb\")\n",
       "print(dataset)\n",
       "```\n",
       "\n",
       "## Roadmap to Using Hugging Face\n",
       "\n",
       "1. **Explore Use Cases**:\n",
       "   - Familiarize yourself with the tasks supported by Hugging Face like Text Generation, Image Classification, etc.\n",
       "\n",
       "2. **Model Deployment**:\n",
       "   - Use Hugging Face's Inference API to serve models in production. Deploy using easy-to-use endpoints.\n",
       "   - Set up a model with a few lines of code to be used in your application.\n",
       "\n",
       "3. **Dataset Management**:\n",
       "   - Access datasets, preprocess them for model training, and share your datasets with the community.\n",
       "\n",
       "4. **Creating Spaces**:\n",
       "   - Build demo applications to showcase models using platforms like [Gradio](https://gradio.app/) and [Streamlit](https://streamlit.io/).\n",
       "\n",
       "5. **Join the Community**:\n",
       "   - Connect with other developers through forums, Discord, and community blogs to share knowledge and resources.\n",
       "\n",
       "## Commands and Scripts Overview\n",
       "\n",
       "Here’s a table summarizing key commands and scripts you'll need when using Hugging Face:\n",
       "\n",
       "| Feature                                 | Command/Script                                                                                      |\n",
       "|-----------------------------------------|-----------------------------------------------------------------------------------------------------|\n",
       "| Install Transformers and Datasets       | `pip install transformers datasets`                                                                 |\n",
       "| Load a Text Generation Model            | ```from transformers import pipeline; model = pipeline(\"text-generation\", model=\"gpt2\")```         |\n",
       "| Load a Dataset                          | ```from datasets import load_dataset; dataset = load_dataset(\"imdb\")```                             |\n",
       "| Use Hugging Face Hub for Inference     | ```from transformers import pipeline; model = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")``` |\n",
       "| Share a Dataset to the Hub              | Use functionalities of the `datasets` library to create a dataset card and upload to the Hub.      |\n",
       "| Build a Web App with Gradio             | ```import gradio as gr; def greet(name): return f\"Hello {name}!\"; gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\").launch()``` |\n",
       "\n",
       "This guide is designed to be user-friendly and to get you up and running with Hugging Face quickly. Happy coding!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_quickstart_guide(\"huggingface\", \"https://huggingface.co/docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5d4de1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found links: {'links': [{'type': 'Installation', 'url': 'https://flask.palletsprojects.com/en/stable/installation/'}, {'type': 'Quickstart', 'url': 'https://flask.palletsprojects.com/en/stable/quickstart/'}, {'type': 'Tutorial', 'url': 'https://flask.palletsprojects.com/en/stable/tutorial/'}, {'type': 'Debugging', 'url': 'https://flask.palletsprojects.com/en/stable/debugging/'}, {'type': 'Testing', 'url': 'https://flask.palletsprojects.com/en/stable/testing/'}, {'type': 'Deploying', 'url': 'https://flask.palletsprojects.com/en/stable/deploying/'}, {'type': 'Templates', 'url': 'https://flask.palletsprojects.com/en/stable/templating/'}, {'type': 'Error Handling', 'url': 'https://flask.palletsprojects.com/en/stable/errorhandling/'}, {'type': 'Configuration', 'url': 'https://flask.palletsprojects.com/en/stable/config/'}, {'type': 'Blueprints', 'url': 'https://flask.palletsprojects.com/en/stable/blueprints/'}]}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Quickstart Guide to Flask\n",
       "\n",
       "## What is Flask?\n",
       "Flask is a lightweight WSGI web application framework designed to make setting up web applications quick and easy. It allows developers to build complex applications with minimal overhead and provides the foundation for web development.\n",
       "\n",
       "## Prerequisites\n",
       "- **Python Version**: Ensure you have Python 3.9 or newer installed.\n",
       "- **Dependencies**: Flask has several dependencies, such as Werkzeug, Jinja, and Click, which will be automatically installed during installation.\n",
       "\n",
       "## Installation Steps\n",
       "\n",
       "### 1. Set Up a Virtual Environment\n",
       "Creating a virtual environment helps manage dependencies specific to your project. Follow the steps below:\n",
       "\n",
       "#### On macOS/Linux:\n",
       "```bash\n",
       "mkdir myproject\n",
       "cd myproject\n",
       "python3 -m venv .venv\n",
       "source .venv/bin/activate\n",
       "```\n",
       "\n",
       "#### On Windows:\n",
       "```bash\n",
       "mkdir myproject\n",
       "cd myproject\n",
       "py -3 -m venv .venv\n",
       ".venv\\Scripts\\activate\n",
       "```\n",
       "\n",
       "### 2. Install Flask\n",
       "Once your virtual environment is active, install Flask using pip:\n",
       "```bash\n",
       "pip install Flask\n",
       "```\n",
       "\n",
       "## Creating a Minimal Application\n",
       "To create a simple Flask application, follow these steps:\n",
       "\n",
       "1. Create a new file named `hello.py` and add the following code:\n",
       "   ```python\n",
       "   from flask import Flask\n",
       "\n",
       "   app = Flask(__name__)\n",
       "\n",
       "   @app.route(\"/\")\n",
       "   def hello_world():\n",
       "       return \"<p>Hello, World!</p>\"\n",
       "   ```\n",
       "\n",
       "2. Run the application:\n",
       "   ```bash\n",
       "   flask --app hello run\n",
       "   ```\n",
       "   This command starts the development server, and your app will be accessible at `http://127.0.0.1:5000/`.\n",
       "\n",
       "### 3. Debug Mode\n",
       "To enable debug mode (good for development):\n",
       "```bash\n",
       "flask --app hello run --debug\n",
       "```\n",
       "This allows automatic reloading and shows detailed error messages.\n",
       "\n",
       "## Key Features\n",
       "### Routing\n",
       "- Use the `@app.route()` decorator to map URLs to functions.\n",
       "  ```python\n",
       "  @app.route('/hello')\n",
       "  def hello():\n",
       "      return 'Hello, World!'\n",
       "  ```\n",
       "\n",
       "### Dynamic URL Handling\n",
       "- Define variable parts in routes:\n",
       "  ```python\n",
       "  @app.route('/user/<username>')\n",
       "  def show_user_profile(username):\n",
       "      return f'User {username}'\n",
       "  ```\n",
       "\n",
       "### Rendering HTML\n",
       "- Use HTML escaping to prevent XSS:\n",
       "  ```python\n",
       "  from markupsafe import escape\n",
       "  \n",
       "  @app.route('/hello/<name>')\n",
       "  def hello(name):\n",
       "      return f'Hello, {escape(name)}!'\n",
       "  ```\n",
       "\n",
       "### Serving Static Files\n",
       "- Place your CSS and JavaScript in a `static` folder.\n",
       "\n",
       "### URL Building\n",
       "- Utilize `url_for()` for generating URLs:\n",
       "  ```python\n",
       "  from flask import url_for\n",
       "\n",
       "  @app.route('/home')\n",
       "  def home():\n",
       "      return redirect(url_for('index'))\n",
       "  ```\n",
       "\n",
       "## Roadmap to Using Flask\n",
       "1. **Installation**: Set up Python and install Flask in a virtual environment.\n",
       "2. **Create Your First App**: Write simple routes and a minimal app structure.\n",
       "3. **Learn Routing**: Explore how to handle various URL patterns and use parameterized routing.\n",
       "4. **HTML Rendering**: Understand how to return HTML content safely and use templates.\n",
       "5. **Debugging & Logging**: Make your app robust by learning debugging techniques and logging errors.\n",
       "6. **Deployment**: Investigate options for deploying your app to production.\n",
       "\n",
       "## Common Commands and Scripts\n",
       "\n",
       "| Command                          | Description                                           |\n",
       "|----------------------------------|-------------------------------------------------------|\n",
       "| `flask --app hello run`         | Run your Flask application.                           |\n",
       "| `flask run --host=0.0.0.0`      | Make the server publicly accessible.                  |\n",
       "| `pip install Flask`              | Install Flask in the virtual environment.             |\n",
       "| `source .venv/bin/activate`     | Activate your virtual environment (macOS/Linux).     |\n",
       "| `.venv\\Scripts\\activate`        | Activate your virtual environment (Windows).          |\n",
       "| `flask --app hello run --debug` | Run the app in debug mode for development.            |\n",
       "\n",
       "By following these instructions, you'll have a working Flask application and a solid foundation to build upon for more complex projects. Enjoy building with Flask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_quickstart_guide(\"Flask\", \"https://flask.palletsprojects.com/en/stable/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ba214d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found links: {'links': [{'type': 'Installation', 'url': 'https://www.tensorflow.org/install'}, {'type': 'Learn', 'url': 'https://www.tensorflow.org/learn'}, {'type': 'Tutorials', 'url': 'https://www.tensorflow.org/tutorials'}, {'type': 'Guides', 'url': 'https://www.tensorflow.org/guide'}, {'type': 'Learn ML resources', 'url': 'https://www.tensorflow.org/resources/learn-ml'}, {'type': 'Datasets', 'url': 'https://www.tensorflow.org/datasets'}, {'type': 'Basics Guide', 'url': 'https://www.tensorflow.org/guide/basics'}, {'type': 'Model Garden Guide', 'url': 'https://www.tensorflow.org/guide/model_garden'}, {'type': 'TensorBoard', 'url': 'https://www.tensorflow.org/tensorboard'}, {'type': 'Keras Preprocessing Layers', 'url': 'https://www.tensorflow.org/guide/keras/preprocessing_layers'}, {'type': 'TFX Guide', 'url': 'https://www.tensorflow.org/tfx'}, {'type': 'Resources for Models and Datasets', 'url': 'https://www.tensorflow.org/resources/models-datasets'}]}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Quickstart Guide to TensorFlow\n",
       "\n",
       "TensorFlow is an open-source machine learning library designed to make it easy for both beginners and experts to create machine learning models for desktop, mobile, web, and cloud applications. This quickstart guide will walk you through the installation steps, basic usage, and resources to enhance your TensorFlow skills.\n",
       "\n",
       "## Table of Contents\n",
       "- [Installation](#installation)\n",
       "- [Basic Usage](#basic-usage)\n",
       "- [Getting Help and Learning Resources](#getting-help-and-learning-resources)\n",
       "- [Important Commands and Scripts](#important-commands-and-scripts)\n",
       "\n",
       "## Installation\n",
       "\n",
       "To install TensorFlow, you can use one of the following methods:\n",
       "\n",
       "### 1. Using pip\n",
       "\n",
       "TensorFlow requires Python 3.8 to 3.11. Ensure you have the latest version of pip before installing.\n",
       "\n",
       "```bash\n",
       "# Upgrade pip\n",
       "pip install --upgrade pip\n",
       "\n",
       "# Install TensorFlow (Stable Version)\n",
       "pip install tensorflow\n",
       "\n",
       "# Or install the preview build (Unstable)\n",
       "pip install tf-nightly\n",
       "```\n",
       "\n",
       "### 2. Using Docker\n",
       "\n",
       "You can also run TensorFlow in a Docker container, which is useful for GPU support.\n",
       "\n",
       "```bash\n",
       "# Pull the latest TensorFlow image\n",
       "docker pull tensorflow/tensorflow:latest\n",
       "\n",
       "# Start Jupyter server in the Docker container\n",
       "docker run -it -p 8888:8888 tensorflow/tensorflow:latest-jupyter\n",
       "```\n",
       "\n",
       "### 3. Using Google Colab\n",
       "\n",
       "If you prefer not to install anything locally, you can run TensorFlow in a web browser through Google Colab, which provides a cloud-based Jupyter notebook environment.\n",
       "\n",
       "## Basic Usage\n",
       "\n",
       "### 1. Import TensorFlow\n",
       "\n",
       "```python\n",
       "import tensorflow as tf\n",
       "```\n",
       "\n",
       "### 2. Create and Train a Simple Model\n",
       "\n",
       "Here's a simple example to create and train a model for classifying images using Keras:\n",
       "\n",
       "```python\n",
       "from tensorflow import keras\n",
       "from tensorflow.keras import layers\n",
       "\n",
       "# Define the model\n",
       "model = keras.Sequential([\n",
       "    layers.Flatten(input_shape=(28, 28)),  # Input layer\n",
       "    layers.Dense(128, activation='relu'),   # Hidden layer\n",
       "    layers.Dense(10, activation='softmax')  # Output layer\n",
       "])\n",
       "\n",
       "# Compile the model\n",
       "model.compile(optimizer='adam',\n",
       "              loss='sparse_categorical_crossentropy',\n",
       "              metrics=['accuracy'])\n",
       "\n",
       "# Load dataset (for example, MNIST)\n",
       "mnist = keras.datasets.mnist\n",
       "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
       "\n",
       "# Normalize the images\n",
       "train_images = train_images / 255.0\n",
       "test_images = test_images / 255.0\n",
       "\n",
       "# Train the model\n",
       "model.fit(train_images, train_labels, epochs=5)\n",
       "```\n",
       "\n",
       "### 3. Evaluate the Model\n",
       "\n",
       "```python\n",
       "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
       "print(f'Test accuracy: {test_acc}')\n",
       "```\n",
       "\n",
       "## Getting Help and Learning Resources\n",
       "\n",
       "- **Official Documentation:** [TensorFlow Documentation](https://www.tensorflow.org/)\n",
       "- **Tutorials:** Explore numerous tutorials for different skill levels [here](https://www.tensorflow.org/tutorials).\n",
       "- **Community Support:** Join forums and user groups to ask questions and share experiences.\n",
       "\n",
       "## Important Commands and Scripts\n",
       "\n",
       "| Command/Script                                  | Description                                          |\n",
       "|------------------------------------------------|------------------------------------------------------|\n",
       "| `pip install tensorflow`                        | Installs the latest stable version of TensorFlow.   |\n",
       "| `pip install tf-nightly`                       | Installs the latest nightly build (unstable).       |\n",
       "| `docker run -it -p 8888:8888 tensorflow/tensorflow:latest-jupyter` | Starts a Jupyter server in Docker.            |\n",
       "| `import tensorflow as tf`                       | Imports the TensorFlow library.                     |\n",
       "| `model.compile()`                              | Configures the model for training.                 |\n",
       "| `model.fit()`                                  | Trains the model on the dataset.                   |\n",
       "| `model.evaluate()`                              | Evaluates the model on the test dataset.           |\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "With these instructions, you should be able to install TensorFlow and start building your own machine learning models. Explore the provided resources to dive deeper into the vast capabilities of TensorFlow. Happy coding!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_quickstart_guide(\"Tensorflow\", \"https://www.tensorflow.org/learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ade223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
